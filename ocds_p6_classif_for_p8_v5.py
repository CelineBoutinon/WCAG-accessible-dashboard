# -*- coding: utf-8 -*-
"""OCDS_P6_classif_for_P8_v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gCb7OPtAzV5BkrQFWv_Kpix3KNbJIhXh

**This notebook won't run in a Windows environment - use Google Colab.**

# !! DO NOT RERUN BEGINNING OF NOTEBOOK - RUN ONLY FROM SECTION 4 BELOW !!!
"""



"""# 1 - Introduction

## 1.1 - Import packages & librairies
"""

# run cell below first when restarting runtime in Google Colab
!pip install nlpaug plot_keras_history

# utilities
import sys
import datetime
from datetime import datetime
import random
import time
import logging
logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere
import os
import shutil
os.environ["TF_KERAS"]='1'
os.environ["TF_XLA_FLAGS"] = "--tf_xla_enable_xla_devices=false"
os.environ["OMP_NUM_THREADS"] = '1'  # needed to avoid memory leak warning with K-Means in Windows environment
from os import listdir
from glob import glob
from timeit import default_timer as timer

# data cleaning & processing
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

# dataviz
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
import plotly.express as px
from matplotlib.ticker import StrMethodFormatter
from matplotlib.ticker import FormatStrFormatter
from plot_keras_history import show_history, plot_history

# text processing
import re
import nltk
from nltk.tokenize import word_tokenize, RegexpTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from collections import defaultdict
from nltk.stem import PorterStemmer, WordNetLemmatizer
from collections import Counter
from wordcloud import WordCloud

# text augmentation
import nlpaug.augmenter.word as naw

# image processing
import cv2
from PIL import Image

# image augmentation
import albumentations as A
from albumentations.pytorch import ToTensorV2

# modelisation
from sklearn import cluster, metrics, manifold, decomposition
from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import lightgbm
from lightgbm import LGBMClassifier
import xgboost as xgb
from xgboost import XGBClassifier
import umap
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten, Dense, Dropout
from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.applications import Xception, InceptionV3
from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess
from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess

# metrics
from sklearn import metrics
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, make_scorer, fbeta_score, precision_score, recall_score

# set dataframe display options
pd.set_option('max_colwidth', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: '%.4f' % x) # Suppress scientific notation and show only 4 decimals
# pd.set_option('display.float_format', lambda x: '%.f' % x) # Suppress scientific notation and show only integer part

# silence warnings after checking
import warnings
# pd.set_option('future.no_silent_downcasting', False) # introduced in pandas 2.0.0., this notebook uses 1.4.4
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=UserWarning)
# warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning) # introduced in pandas 2.0.0., this notebook uses 1.4.4
# from PIL import ImageDecompressionBombWarning
warnings.simplefilter('ignore', Image.DecompressionBombWarning)

# extract colors from logo for ppt slideshow
# banana = findColor('banana.png')
# print("banana hex :", banana)
banana = '#fcf7c9'

viridis_sample = ['#481567FF','#453781FF','#39568CFF','#2D708EFF','#238A8DFF','#20A387FF','#3CBB75FF', '#73D055FF','#B8DE29FF']

viridis_palette = ['#440154', '#481e70', '#443982', '#3a528b', '#30678d', '#287b8e', '#20908c', '#20a485', '#35b778', '#5ec961',
                   '#90d643', '#c7e01f', '#fde724']

sunset_palette = ["#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00", "#FF5722", "#FF3D00", "#FF2D00",
                  "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081", "#F50057", "#D5006D", "#C51162"]

palette = ['#440154', '#481e70', '#443982', '#3a528b', '#30678d', '#287b8e', '#20908c', '#20a485', '#35b778', '#5ec961',
           '#90d643', '#c7e01f', '#fde724', "#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00",
           "#FF5722", "#FF3D00", "#FF2D00", "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081",
           "#F50057", "#D5006D", "#C51162"]

# run this cell in Google Colab only
from google.colab import drive
drive.mount('/content/drive')
image_path = '/content/drive/My Drive/Colab Notebooks/OCDS_P6/flipkart_images'
image_path_aug = '/content/drive/My Drive/Colab Notebooks/OCDS_P6/augmented_images'
save_path = '/content/drive/My Drive/Colab Notebooks/OCDS_P6'
image_save_path = save_path

# import custom user-defined functions
functions_path = os.path.join(save_path, 'functions.py')

# Check if the path is already in sys.path
if os.path.dirname(functions_path) not in sys.path:
    sys.path.append(os.path.dirname(functions_path))

from functions import *

# import split df with photo names, pre-processed text and product categories
train_df = pd.read_parquet(save_path + '/'+'train_df.gzip')
y_train = train_df['real_clusters']
val_df = pd.read_parquet(save_path + '/'+'val_df.gzip')
y_val = val_df['real_clusters']
test_df = pd.read_parquet(save_path + '/'+'test_df.gzip')
y_test = test_df['real_clusters']
train_val_df = pd.concat([train_df, val_df], axis=0, ignore_index=True)
y_train_val_images = pd.concat([y_train, y_val], axis=0, ignore_index=True)

# import merged text and image features
X_train_aug = pd.read_csv(save_path + '/' + 'X_train_aug.csv')
X_train_no_aug = pd.read_csv(save_path + '/' + 'X_train_no_aug.csv')
X_val_aug = pd.read_csv(save_path + '/' + 'X_val_aug.csv')
X_val_no_aug = pd.read_csv(save_path + '/' + 'X_val_no_aug.csv')
X_test_aug = pd.read_csv(save_path + '/' + 'X_test_aug.csv')
X_test_no_aug = pd.read_csv(save_path + '/' + 'X_test_no_aug.csv')
# create / import datasets for best model re-training
# X_train_val_aug = pd.concat([X_train_aug, X_val_aug], axis=0, ignore_index=True)
# print(X_train_val_aug.shape)
# X_train_val_aug.to_csv(save_path + '/' + 'X_traing_val_aug.csv', index=False)
X_train_val_aug = pd.read_csv(save_path + '/' + 'X_traing_val_aug.csv')
# X_train_val_no_aug = pd.concat([X_train_no_aug, X_val_no_aug], axis=0, ignore_index=True)
# print(X_train_val_no_aug.shape)
# X_train_val_no_aug.to_csv(save_path + '/' + 'X_traing_val_no_aug.csv', index=False)
X_train_val_no_aug = pd.read_csv(save_path + '/' + 'X_traing_val_no_aug.csv')
# y_train_val = pd.concat([y_train, y_val], axis=0, ignore_index=True)
# print(y_train_val.shape)
# y_train_val.to_csv(save_path + '/' + 'y_train_val.csv', index=False)
y_train_val = pd.read_csv(save_path + '/' + 'y_train_val.csv')

# import text features
text_features_train_aug = pd.read_csv(save_path + '/'+ 'text_features_train_aug.csv')
text_features_train_no_aug = pd.read_csv(save_path + '/'+ 'text_features_train_no_aug.csv')
text_features_val_aug = pd.read_csv(save_path + '/'+'text_features_val_aug.csv')
text_features_val_no_aug = pd.read_csv(save_path + '/'+'text_features_val_no_aug.csv')
text_features_test_aug = pd.read_csv(save_path + '/'+'text_features_test_aug.csv')
text_features_test_no_aug = pd.read_csv(save_path + '/'+'text_features_test_no_aug.csv')

# import image features
image_features_train_aug = pd.read_csv(save_path + '/' + 'image_features_train_aug.csv')
image_features_train_no_aug = pd.read_csv(save_path + '/' + 'image_features_train_no_aug.csv')
image_features_val = pd.read_csv(save_path + '/'+ 'image_features_val.csv')
image_features_test = pd.read_csv(save_path + '/'+ 'image_features_test.csv')

print(np.__version__, '\n')
print(tf.__file__, '\n')
print(tf.__version__, '\n')
print(hasattr(tf, 'keras'), '\n')
print(type(tf), '\n')
print(tf.__spec__, '\n')
print(dir(tf.keras))  # should list keras submodules

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"TensorFlow detected {len(gpus)} GPU(s):")
    for gpu in gpus:
        print(f" - {gpu}")
else:
    print("TensorFlow did NOT detect any GPUs.")

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print(tf.test.is_built_with_cuda())

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# initialise random state for all models and transformers
rs_list = [8, 13, 42]
rs = rs_list[random.randrange(len(rs_list))]
print("Random state =", rs)



"""## 1.2 - Define text functions

### 1.2.1 - Text augmentation
"""

# Initialize augmenters once
synonym_aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.1)  # Replace ~10% words with synonyms
random_insertion_aug = naw.RandomWordAug(action='insert', aug_p=0.1)  # Insert random words
contextual_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action='insert', aug_p=0.1)
random_word_aug = naw.RandomWordAug(action='swap', aug_p=0.1)  # swap words instead of insert
random_word_aug = naw.RandomWordAug(action='delete', aug_p=0.1)  # delete words instead of insert

def augment_text(text):
    # Apply synonym replacement
    augmented_text = synonym_aug.augment(text)
    # Apply random insertion
    # augmented_text = random_insertion_aug.augment(augmented_text) # nlpaug bug - currently HS
    augmented_text = naw.RandomWordAug(action='swap', aug_p=0.1)
    augmented_text = contextual_aug.augment(augmented_text)
    return augmented_text

def augment_corpus(df, text_column, label_column):
    augmented_texts = []
    labels = []

    for idx, row in df.iterrows():
        original_text = row[text_column]
        label = row[label_column]

        # Augment the text
        augmented_text = augment_text(original_text)

        augmented_texts.append(augmented_text)
        labels.append(label)

    augmented_df = pd.DataFrame({
        'augmented_text': augmented_texts,
        'label': labels
    })
    return augmented_df



"""### 1.2.2 - Text processing"""

def tokenize(word_string):
    tokenizer = RegexpTokenizer(r"[a-zA-Z]+")
    word_tokens = tokenizer.tokenize(word_string.lower())
    return word_tokens

def remove_stop_words(word_list):
    stop_words = list(set(stopwords.words('english')))
    filtered_words = [word for word in word_list if not word in stop_words]
    filtered_words_trim = [word for word in filtered_words if len(word) > 2]
    return filtered_words_trim

def lemmatize(word_list):
    lemmatizer = WordNetLemmatizer()
    lem_words = [lemmatizer.lemmatize(word) for word in word_list]
    return lem_words

def stem(word_list):
    stemmer = PorterStemmer()
    stem_words = [stemmer.stem(word) for word in word_list]
    return stem_words

def filter_words(word_list, words_to_trim):
    filtered_words = [word for word in word_list if not word in words_to_trim]
    # filtered_w2 = [w for w in filtered_w if len(w) > 2]
    return filtered_words

# Sub-function to remove specific words from a list of tokens
def trim_words_fct(tokens, words_to_trim):
    words_to_trim_set = set(words_to_trim)
    return [token for token in tokens if token not in words_to_trim_set]

# Text prep function for Bag-of-Words, Tf-idf & Word2Vec
def bag_of_words_transfo(word_string):
    word_tokens = tokenize(word_string)
    sw = remove_stop_words(word_tokens)
    trimmed = trim_words_fct(sw, words_to_trim)
    # lem_w = lemmatize(sw)
    transf_desc_text = ' '.join(trimmed)
    return transf_desc_text

# Text prep function for Bag-of-Words with lemmatization
def bag_of_words_transfo_lem(word_string):
    word_tokens = tokenize(word_string)
    sw = remove_stop_words(word_tokens)
    lem_w = lemmatize(sw)
    trimmed = trim_words_fct(lem_w, words_to_trim)
    transf_desc_text = ' '.join(trimmed)
    return transf_desc_text

# Text prep function for deep learning (BERT & USE)
def deep_lean_transfo(word_string) :
    word_tokens = tokenize(word_string)
    # sw = remove_stop_words(word_tokens)
    # trimmed = trim_words_fct(lem_w, words_to_trim)
    # lem_w = lemmatize(sw)
    transf_desc_text = ' '.join(word_tokens)
    return transf_desc_text



"""### 1.2.3 - Graphing functions"""

# recoloring function for wordclouds
def sunset_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    return random.choice(sunset_palette)



"""## 1.3 - Define text classification metrics"""

def calc_ari(features, perplexity=30, n_components=2):
    time1 = time.time()
    num_labels = len(np.unique(y_cat_num))
    random_state = 42

    # t-SNE embedding
    tsne = manifold.TSNE(
        n_components=n_components,
        perplexity=perplexity,
        n_iter=2000,
        init='random',
        learning_rate=200,
        random_state=random_state
    )
    X_tsne = tsne.fit_transform(features)

    # UMAP embedding
    reducer = umap.UMAP(
        n_components=n_components,
        n_neighbors=15,
        min_dist=0.1,
        random_state=random_state
    )
    X_umap = reducer.fit_transform(features)

    # Clustering on t-SNE embedding
    cls = cluster.KMeans(n_clusters=num_labels, n_init=100, random_state=random_state)
    cls.fit(X_tsne)

    ARI = np.round(metrics.adjusted_rand_score(y_cat_num, cls.labels_), 4)
    fit_time = np.round(time.time() - time1, 2)
    print(f"ARI: {ARI}, Time: {fit_time}s")

    return ARI, X_tsne, X_umap, cls.labels_, fit_time

def find_optimum_perplexity(features):
    perplexity_range = np.arange(5, 55, 5)
    divergence = []

    for p in perplexity_range:
        model = TSNE(n_components=2, init="pca", perplexity=p, random_state=42)
        _ = model.fit_transform(features)
        divergence.append(model.kl_divergence_)

    # Prepare data for seaborn
    data_plot = pd.DataFrame({'Perplexity': perplexity_range, 'KL Divergence': divergence})
    sns.set(rc={'figure.figsize':(6, 4), 'axes.facecolor':'white', 'figure.facecolor':'gainsboro'})

    sns.lineplot(x='Perplexity', y='KL Divergence', data=data_plot, marker="o", color=banana, linewidth=2)
    plt.title("t-SNE KL Divergence vs Perplexity")
    plt.xlabel("Perplexity Values")
    plt.ylabel("KL Divergence")
    plt.xticks(perplexity_range)

    # Generate timestamp and save plot
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"optimum_perplexity_{timestamp}.png"
    plt.savefig(save_path + '/' + filename)

    plt.show()

    # Find perplexity with minimum divergence
    min_index = np.argmin(divergence)
    optimum_perplexity = perplexity_range[min_index]

    print(f"Optimum perplexity: {optimum_perplexity} with divergence {divergence[min_index]:.4f}")
    return optimum_perplexity

def TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI):
    """
    Visualize t-SNE embeddings colored by true categories and by cluster labels,
    using a custom color palette for both.

    Parameters:
    - X_tsne: np.ndarray of shape (n_samples, 2)
    - y_cat_num: array-like of true category numbers (ints)
    - labels: array-like of cluster numbers (ints)
    - ARI: float, Adjusted Rand Index
    - sunset_palette: list of color hex codes
    - products_trim: DataFrame with column 'real_clusters' (category names)
    """
    # Get unique category names in the order of their numeric encoding
    # (Assume y_cat_num is label-encoded so 0 maps to first unique, etc.)
    category_names = list(y_cat_num.unique())
    n_categories = len(category_names)
    n_clusters = len(np.unique(labels))

    # Ensure palette is long enough
    sunset_palette = ["#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00", "#FF5722", "#FF3D00", "#FF2D00",
                  "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081", "#F50057", "#D5006D", "#C51162"]
    palette_cat = sunset_palette * ((n_categories // len(sunset_palette)) + 1)
    palette_cluster = sunset_palette * ((n_clusters // len(sunset_palette)) + 1)

    fig = plt.figure(figsize=(15, 6))

    # Left: by true category
    ax1 = fig.add_subplot(121)
    scatter1 = ax1.scatter(
        X_tsne[:, 0], X_tsne[:, 1],
        c=y_cat_num,
        cmap=matplotlib.colors.ListedColormap(palette_cat[:n_categories]),
        s=40, alpha=0.85
    )
    # Custom legend for categories
    handles1 = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette_cat[i], markersize=10)
                for i in range(n_categories)]
    ax1.legend(handles1, category_names, loc="best", title="Category")
    ax1.set_title('t-SNE by True Categories')

    # Right: by cluster label
    ax2 = fig.add_subplot(122)
    scatter2 = ax2.scatter(
        X_tsne[:, 0], X_tsne[:, 1],
        c=labels,
        cmap=matplotlib.colors.ListedColormap(palette_cluster[:n_clusters]),
        s=40, alpha=0.85
    )
    # Custom legend for clusters
    cluster_labels = [f'Cluster {i}' for i in range(n_clusters)]
    handles2 = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette_cluster[i], markersize=10)
                for i in range(n_clusters)]
    ax2.legend(handles2, cluster_labels, loc="best", title="Cluster")
    ax2.set_title('t-SNE by Clusters')

    plt.suptitle(f"t-SNE Visualization | ARI: {ARI}", fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
    print("ARI:", ARI)

def UMAP_visu_fct(X_umap, y_cat_num, labels, ARI):
    """
    Visualize UMAP embeddings colored by true categories and by cluster labels,
    using a custom color palette for both.

    Parameters:
    - X_umap: np.ndarray of shape (n_samples, 2), UMAP embedding
    - y_cat_num: array-like of true category numbers (ints)
    - labels: array-like of cluster numbers (ints)
    - ARI: float, Adjusted Rand Index
    """
    category_names = np.unique(y_cat_num)
    n_categories = len(category_names)
    n_clusters = len(np.unique(labels))

    sunset_palette = [
        "#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00", "#FF5722", "#FF3D00", "#FF2D00",
        "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081", "#F50057", "#D5006D", "#C51162"
    ]
    palette_cat = sunset_palette * ((n_categories // len(sunset_palette)) + 1)
    palette_cluster = sunset_palette * ((n_clusters // len(sunset_palette)) + 1)

    fig = plt.figure(figsize=(15, 6))

    # Left: by true category
    ax1 = fig.add_subplot(121)
    scatter1 = ax1.scatter(
        X_umap[:, 0], X_umap[:, 1],
        c=y_cat_num,
        cmap=matplotlib.colors.ListedColormap(palette_cat[:n_categories]),
        s=40, alpha=0.85
    )
    handles1 = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette_cat[i], markersize=10)
                for i in range(n_categories)]
    ax1.legend(handles1, category_names, loc="best", title="Category")
    ax1.set_title('UMAP by True Categories')

    # Right: by cluster label
    ax2 = fig.add_subplot(122)
    scatter2 = ax2.scatter(
        X_umap[:, 0], X_umap[:, 1],
        c=labels,
        cmap=matplotlib.colors.ListedColormap(palette_cluster[:n_clusters]),
        s=40, alpha=0.85
    )
    cluster_labels = [f'Cluster {i}' for i in range(n_clusters)]
    handles2 = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette_cluster[i], markersize=10)
                for i in range(n_clusters)]
    ax2.legend(handles2, cluster_labels, loc="best", title="Cluster")
    ax2.set_title('UMAP by Clusters')

    plt.suptitle(f"UMAP Visualization | ARI: {ARI}", fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
    print("ARI:", ARI)



"""## 1.4 - Define image augmentation functions

### 1.4.1 - Image augmentation
"""

# Define augmentation pipeline
transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    # A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), p=0.5), # outdated syntax
    A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), p=0.5), # updated syntax
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.GaussianBlur(blur_limit=(3, 7), p=0.3),
    A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.3),
], p=1.0)

def augment_and_save_train_images(image_path, image_save_path, train_df):
    # Create output directory if it doesn't exist
    save_dir = os.path.join(image_save_path, "augmented_images")
    os.makedirs(save_dir, exist_ok=True)

    # Get the list of image filenames from train_df
    train_image_files = set(train_df['image'].tolist())

    print(f"Found {len(train_image_files)} images in training set to augment.")

    for idx, filename in enumerate(train_image_files):
        img_path = os.path.join(image_path, filename)
        image = cv2.imread(img_path)
        if image is None:
            print(f"Warning: Could not load image {img_path}. Skipping.")
            continue

        # Convert BGR to RGB for Albumentations
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Apply augmentation
        augmented = transform(image=image_rgb)
        aug_image = augmented['image']

        # Convert back RGB to BGR for saving with OpenCV
        aug_image_bgr = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)

        # Add '_aug' suffix before the file extension
        name, ext = os.path.splitext(filename)
        save_filename = f"{name}{ext}"
        save_path = os.path.join(save_dir, save_filename)

        cv2.imwrite(save_path, aug_image_bgr)

        if idx % 100 == 0:
            print(f"Processed {idx}/{len(train_image_files)} images")

    print(f"Augmented training images saved to: {save_dir}")



"""### 1.4.2 - Image processing"""

def extract_features_from_list(model, preprocess_func, list_photos, path, target_size=(299, 299)):
    features = []
    start_time = time.time()
    for i, photo in enumerate(list_photos):
        if i % 100 == 0:
            print(f"Processing image {i}/{len(list_photos)}")
        img_path = path + '/' + photo  # full path to image
        image = load_img(img_path, target_size=target_size)
        image = img_to_array(image)
        image = np.expand_dims(image, axis=0)
        image = preprocess_func(image)
        feat = model.predict(image, verbose=0)
        features.append(feat[0])
    duration = time.time() - start_time
    print(f"Features creation time: {duration:.2f} secs")
    return np.array(features)

def extract_sift_descriptors(list_photos, path, sift):
    sift_keypoints = []

    for image_num, filename in enumerate(list_photos):
        if image_num % 50 == 0:
           print(f'progress : {image_num / len(list_photos) * 100:.2f} %')

        image_path = os.path.join(path, filename)
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load grayscale

        if image is None:
            print(f"Could not load image: {image_path}")
            sift_keypoints.append(None)  # Keep alignment with input list
            continue

        # Histogram equalization
        equalized = cv2.equalizeHist(image)

        # Detect SIFT keypoints and descriptors
        kp, des = sift.detectAndCompute(equalized, None)

        sift_keypoints.append(des)

    return sift_keypoints

def list_pix(name) :
    list_image_name = [list_photos[i] for i in range(len(data)) if photo_data["product_category"][i]==name]
    return list_image_name

def save_image_set(image_path, image_save_path, val_test_df):
    # Create output directory if it doesn't exist
    save_dir = os.path.join(image_save_path)
    os.makedirs(save_dir, exist_ok=True)

    # Get the list of image filenames from train_df
    image_files = set(val_test_df['image'].tolist())

    print(f"Found {len(image_files)} images in set.")

    for idx, filename in enumerate(image_files):
        img_path = os.path.join(image_path, filename)
        image = cv2.imread(img_path)
        if image is None:
            print(f"Warning: Could not load image {img_path}. Skipping.")
            continue

        # Build the full save path for each image:
        save_file_path = os.path.join(image_save_path, filename)

        cv2.imwrite(save_file_path, image)  # Corrected this line

        if idx % 25 == 0:
          print(f"Processed {idx}/{len(image_files)} images")

    print(f"Images saved to: {save_dir}")

def save_image_subsets(image_path, image_save_path, val_test_df):
    # Ensure the main save directory exists
    os.makedirs(image_save_path, exist_ok=True)

    # Get unique cluster labels
    unique_clusters = val_test_df['real_clusters'].unique()
    print(f"Found {len(val_test_df)} images in set across {len(unique_clusters)} clusters.")

    # Create subdirectories for each cluster
    for cluster in unique_clusters:
        cluster_dir = os.path.join(image_save_path, str(cluster))
        os.makedirs(cluster_dir, exist_ok=True)

    # Iterate over rows and copy images to the appropriate cluster subfolder
    for idx, row in val_test_df.iterrows():
        filename = row['image']
        cluster = row['real_clusters']
        img_path = os.path.join(image_path, filename)
        save_dir = os.path.join(image_save_path, str(cluster))
        save_file_path = os.path.join(save_dir, filename)

        image = cv2.imread(img_path)
        if image is None:
            print(f"Warning: Could not load image {img_path}. Skipping.")
            continue

        cv2.imwrite(save_file_path, image)

        if idx % 25 == 0:
          print(f"Processed {idx}/{len(val_test_df)} images")

    print(f"Images saved to subfolders in: {image_save_path}")

def merge_subfolders(folder_a_path, folder_b_path, folder_c_path):
    print(f"Merging files from '{folder_a_path}' and '{folder_b_path}' into '{folder_c_path}'...")

    # Ensure the destination parent directory exists
    os.makedirs(folder_c_path, exist_ok=True)

    # Get list of subfolders from folder A (assuming B has the same)
    try:
        subfolders_a = [d for d in os.listdir(folder_a_path) if os.path.isdir(os.path.join(folder_a_path, d))]
        print(f"Found subfolders in '{folder_a_path}': {subfolders_a}")
    except FileNotFoundError:
        print(f"Error: Source folder A not found at '{folder_a_path}'")
        return
    except Exception as e:
        print(f"An error occurred while listing subfolders in '{folder_a_path}': {e}")
        return

    # Iterate through each subfolder found in folder A
    for subfolder_name in subfolders_a:
        subfolder_a_full_path = os.path.join(folder_a_path, subfolder_name)
        subfolder_b_full_path = os.path.join(folder_b_path, subfolder_name)
        subfolder_c_full_path = os.path.join(folder_c_path, subfolder_name)

        # Create the corresponding subfolder in the destination folder C
        os.makedirs(subfolder_c_full_path, exist_ok=True)

        print(f"Processing subfolder '{subfolder_name}'...")

        # Copy files from subfolder A to subfolder C
        if os.path.isdir(subfolder_a_full_path):
            try:
                files_in_a = os.listdir(subfolder_a_full_path)
                print(f"Found {len(files_in_a)} files in '{subfolder_a_full_path}'")
                for item_name in files_in_a:
                    source_item_path = os.path.join(subfolder_a_full_path, item_name)
                    destination_item_path = os.path.join(subfolder_c_full_path, item_name)
                    # Only copy if it's a file and doesn't already exist in the destination
                    if os.path.isfile(source_item_path):
                         # Avoid copying if the file name already exists (in case of duplicates)
                        if not os.path.exists(destination_item_path):
                            shutil.copy2(source_item_path, destination_item_path) # copy2 preserves metadata
            except FileNotFoundError:
                 print(f"Warning: Subfolder A '{subfolder_name}' not found at '{subfolder_a_full_path}'. Skipping.")
                 pass # Skip if the subfolder is missing in A
            except Exception as e:
                print(f"An error occurred while copying files from '{subfolder_a_full_path}': {e}")


        # Copy files from subfolder B to subfolder C
        if os.path.isdir(subfolder_b_full_path):
             try:
                files_in_b = os.listdir(subfolder_b_full_path)
                print(f"Found {len(files_in_b)} files in '{subfolder_b_full_path}'")
                for item_name in files_in_b:
                    source_item_path = os.path.join(subfolder_b_full_path, item_name)
                    destination_item_path = os.path.join(subfolder_c_full_path, item_name)
                     # Only copy if it's a file and doesn't already exist in the destination
                    if os.path.isfile(source_item_path):
                        # Avoid copying if the file name already exists (in case of duplicates)
                        if not os.path.exists(destination_item_path):
                            shutil.copy2(source_item_path, destination_item_path) # copy2 preserves metadata
             except FileNotFoundError:
                 print(f"Warning: Subfolder B '{subfolder_name}' not found at '{subfolder_b_full_path}'. Skipping.")
                 pass # Skip if the subfolder is missing in B
             except Exception as e:
                print(f"An error occurred while copying files from '{subfolder_b_full_path}': {e}")

    print("Merging process completed.")

def merge_subfolders_w_dups(folder_a_path, folder_b_path, folder_c_path):
    print(f"Merging files from '{folder_a_path}' and '{folder_b_path}' into '{folder_c_path}'...")

    # Ensure the destination parent directory exists
    os.makedirs(folder_c_path, exist_ok=True)

    # Get list of subfolders from folder A (assuming B has the same)
    try:
        subfolders_a = [d for d in os.listdir(folder_a_path) if os.path.isdir(os.path.join(folder_a_path, d))]
        print(f"Found subfolders in '{folder_a_path}': {subfolders_a}")
    except FileNotFoundError:
        print(f"Error: Source folder A not found at '{folder_a_path}'")
        return
    except Exception as e:
        print(f"An error occurred while listing subfolders in '{folder_a_path}': {e}")
        return

    # Iterate through each subfolder found in folder A
    for subfolder_name in subfolders_a:
        subfolder_a_full_path = os.path.join(folder_a_path, subfolder_name)
        subfolder_b_full_path = os.path.join(folder_b_path, subfolder_name)
        subfolder_c_full_path = os.path.join(folder_c_path, subfolder_name)

        # Create the corresponding subfolder in the destination folder C
        os.makedirs(subfolder_c_full_path, exist_ok=True)

        print(f"Processing subfolder '{subfolder_name}'...")

        # Copy files from subfolder A to subfolder C
        if os.path.isdir(subfolder_a_full_path):
            try:
                files_in_a = os.listdir(subfolder_a_full_path)
                print(f"Found {len(files_in_a)} files in '{subfolder_a_full_path}'")
                for item_name in files_in_a:
                    source_item_path = os.path.join(subfolder_a_full_path, item_name)
                    destination_item_path = os.path.join(subfolder_c_full_path, item_name)
                    # Only copy if it's a file
                    if os.path.isfile(source_item_path):
                        # Check if the file name already exists in the destination
                        base, ext = os.path.splitext(item_name)
                        counter = 1
                        while os.path.exists(destination_item_path):
                            destination_item_path = os.path.join(subfolder_c_full_path, f"{base}_{counter}{ext}")
                            counter += 1
                        shutil.copy2(source_item_path, destination_item_path) # copy2 preserves metadata
            except FileNotFoundError:
                 print(f"Warning: Subfolder A '{subfolder_name}' not found at '{subfolder_a_full_path}'. Skipping.")
                 pass # Skip if the subfolder is missing in A
            except Exception as e:
                print(f"An error occurred while copying files from '{subfolder_a_full_path}': {e}")


        # Copy files from subfolder B to subfolder C
        if os.path.isdir(subfolder_b_full_path):
             try:
                files_in_b = os.listdir(subfolder_b_full_path)
                print(f"Found {len(files_in_b)} files in '{subfolder_b_full_path}'")
                for item_name in files_in_b:
                    source_item_path = os.path.join(subfolder_b_full_path, item_name)
                    destination_item_path = os.path.join(subfolder_c_full_path, item_name)
                     # Only copy if it's a file
                    if os.path.isfile(source_item_path):
                        # Check if the file name already exists in the destination
                        base, ext = os.path.splitext(item_name)
                        counter = 1
                        while os.path.exists(destination_item_path):
                            destination_item_path = os.path.join(subfolder_c_full_path, f"{base}_{counter}{ext}")
                            counter += 1
                        shutil.copy2(source_item_path, destination_item_path) # copy2 preserves metadata
             except FileNotFoundError:
                 print(f"Warning: Subfolder B '{subfolder_name}' not found at '{subfolder_b_full_path}'. Skipping.")
                 pass # Skip if the subfolder is missing in B
             except Exception as e:
                print(f"An error occurred while copying files from '{subfolder_b_full_path}': {e}")

    print("Merging process completed.")



"""### 1.4.3 - Graphing functions"""

def plot_pca_scree(explained_variance_ratio, cumulative_variance, components,
                   num_components_99, num_component_threshold,
                   num_component_kaiser, cumulative_variance_kaiser,
                   max_ticks=10,
                   save_path=None):

    sns.set(rc={'figure.figsize': (7, 4), 'axes.facecolor': 'white', 'figure.facecolor': 'gainsboro'})

    step = max(1, len(components) // max_ticks)
    xticks_to_show = components[::step]

    # Define colors
    individual_color = 'cornflowerblue'
    cumulative_color = banana
    line_99_color = 'limegreen'
    line_kaiser_color = 'fuchsia'

    plt.plot(components, explained_variance_ratio * 100,
             linewidth=2, color=individual_color, label='Individual Explained Variance')
    plt.plot(components, cumulative_variance * 100,
             linewidth=2, color=cumulative_color, label='Cumulative Explained Variance')

    plt.axhline(y=99, color=line_99_color, linestyle='--',
                label='99% Variance Threshold', linewidth=0.5)

    plt.axhline(y=cumulative_variance_kaiser, color=line_kaiser_color, linestyle='--',
                label=f'Kaiser Variance {cumulative_variance_kaiser}% ', linewidth=0.5)

    plt.axvline(x=num_components_99, color=line_99_color, linestyle='-.',
                label=f'{num_components_99} Components for 99% Variance', linewidth=0.5)

    plt.axvline(x=num_component_threshold, color=line_kaiser_color, linestyle='-.',
                label=(f"Kaiser's rule ({num_component_kaiser} components)\n"
                       f"{cumulative_variance_kaiser} % explained variance"),
                       linewidth=0.5)

    plt.xlabel('Principal Component', fontsize=10, fontweight='bold')
    plt.ylabel('Explained Variance (%)', fontsize=10, fontweight='bold')
    plt.title('Scree Plot with Variance Thresholds', fontsize=16, fontweight='bold')
    plt.xticks(xticks_to_show)
    plt.legend()
    plt.grid(axis='y', color='gainsboro')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)
    plt.show()

def plot_tsne_clusters(df_tsne, save_path=None):
    sns.set(rc={'figure.figsize': (8, 5), 'axes.facecolor': 'white', 'figure.facecolor': 'gainsboro'})
    sns.scatterplot(x="tsne1", y="tsne2", hue="cat_clusters", data=df_tsne, legend="brief", palette=sns.color_palette('Set2', n_colors=7),
                    s=50, alpha=0.5)
    plt.title('t-SNE - product categories', fontsize=14, fontweight='bold')
    plt.xlabel('t-SNE 1', fontsize=10, fontweight='bold')
    plt.ylabel('t-SNE 2', fontsize=10, fontweight='bold')
    plt.axhline(y=0, color='gainsboro', linewidth=1)
    plt.axvline(x=0, color='gainsboro', linewidth=1)
    plt.legend(prop={'size': 10})
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path)
    plt.show()

def plot_tsne_kmeans_clusters(df_tsne, save_path=None):
    plt.figure(figsize=(8,5))
    sns.scatterplot(x="tsne1", y="tsne2", hue='cluster_kmeans', palette=sns.color_palette('Set2', n_colors=7), s=50, alpha=0.5,
                    data=df_tsne, legend="brief")
    plt.axhline(y=0, color='gainsboro', linewidth=1)
    plt.axvline(x=0, color='gainsboro', linewidth=1)
    plt.title('t-SNE - K-Means clusters ', fontsize=14, fontweight='bold')
    plt.xlabel('t-SNE 1', fontsize=10, fontweight='bold')
    plt.ylabel('t-SNE 2', fontsize=10, fontweight='bold')
    plt.legend(prop={'size': 9})
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)
    plt.show()

def compute_tsne(feat_pca, photo_data, random_state=None):
    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,
                        init='random', random_state=random_state)
    X_tsne = tsne.fit_transform(feat_pca)

    df_tsne = pd.DataFrame(X_tsne[:, :2], columns=['tsne1', 'tsne2'])
    df_tsne["real_clusters"] = photo_data['real_clusters'].values
    df_tsne["cat_clusters"] = photo_data['real_clusters'].astype(str) + " - " + photo_data['product_category']

    print(f"t-SNE DataFrame shape: {df_tsne.shape}")
    return df_tsne



"""## 1.5 - Define image classification metrics"""

def pca_analysis(im_features, random_state=None):
    pca = PCA(random_state=random_state)
    pca.fit(im_features)

    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance_ratio)
    components = np.arange(1, len(explained_variance_ratio) + 1)

    num_components_99 = np.argmax(cumulative_variance >= 0.99) + 1
    threshold = 1 / len(explained_variance_ratio)
    num_component_threshold = np.argmax(explained_variance_ratio < threshold) + 1

    # Kaiser criterion: components with eigenvalue > average eigenvalue (threshold)
    num_component_kaiser = np.argmax(explained_variance_ratio < threshold) + 1
    cumulative_variance_kaiser = round(np.sum(explained_variance_ratio[:num_component_kaiser]) * 100, 2)

    return (explained_variance_ratio, cumulative_variance, components,
            num_components_99, threshold, num_component_threshold,
            num_component_kaiser, cumulative_variance_kaiser)



"""## 1.6 - Import & split data"""

category_mapping = pd.read_csv(save_path + '/' +'category_mapping.csv')
category_mapping

products_trim_final = pd.read_parquet(save_path + '/' +'products_trim_final.parquet.gzip')
# products_trim_final.head(1)

feat = products_trim_final['corpus_deep_learn']
# y_cat_num = products_trim_final['real_clusters']

# First split: train (70%) and temp (30%)
train_df, temp_df = train_test_split(products_trim_final, test_size=(350/1050), stratify=products_trim_final['real_clusters'],
                                     random_state=rs)

# Second split: validation (15%) and test (15%) from temp (30%)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['real_clusters'], random_state=rs)

# Check the sizes
print(f"Train size: {len(train_df)}")
print(f"Validation size: {len(val_df)}")
print(f"Test size: {len(test_df)}")

print("Train class distribution:")
train_df['real_clusters'].value_counts()

print("Validation class distribution:")
val_df['real_clusters'].value_counts()

print("Test class distribution:")
test_df['real_clusters'].value_counts()

train_df.to_parquet('train_df.gzip', compression='gzip')
val_df.to_parquet('val_df.gzip', compression='gzip')
test_df.to_parquet('test_df.gzip', compression='gzip')



"""## 1.7 - Data augmentation

### 1.7.1 - Text data
"""

# Augment only training data
augmented_train_df = augment_corpus(train_df, 'corpus_bow_lem', 'real_clusters')
print(augmented_train_df.shape)
augmented_train_df.head()

augmented_train_df.to_parquet('augmented_text_train_df.gzip', compression='gzip')



"""### 1.7.2 - Images

#### 1.7.2.1 - Training images
"""

# train_df = pd.read_parquet(save_path + '/'+'train_df.gzip')

# !!!Empty directory before regenerating!!!
# augment_and_save_train_images(image_path, image_save_path, train_df)



"""#### 1.7.2.2 - Training + validation images"""

# !!!Empty directory before regenerating!!!
# augment_and_save_train_images(image_path, image_save_path, train_val_df)

# Generate subsets only once - remove folder before re-generating
# save_image_subsets(image_save_path + '/' +'augmented_images_train_val', image_save_path + '/' + 'train_val_images_subsets_aug', val_test_df=train_val_df)



"""### 1.7.3 - Create grouped subfolders for ResNet50 re-training"""

# Create grouped subfolders for train + val images
# source_folder_1 = image_save_path + '/' + 'train_images_subsets'
# source_folder_2 = image_save_path + '/' + 'val_images_subsets'
# destination_folder_1 = image_save_path + '/' + 'train_val_images_subsets'
# merge_subfolders(source_folder_1, source_folder_2, destination_folder_1)

# Create grouped subfolders for (train + val images) + (train + val images) augmented
# source_folder_3 = image_save_path + '/' + 'train_val_images_subsets'
# source_folder_4 = image_save_path + '/' + 'train_val_images_subsets_aug'
# destination_folder_2 = image_save_path + '/' + 'train_val_images_subsets_ALL'
# merge_subfolders_w_dups(source_folder_3, source_folder_4, destination_folder_2)





"""## 1.8 - Generate features

### 1.8.1 - Text data - using Tf-idf

#### 1.8.1.1 - Training text data

#### 1.8.1.1-a - With data augmentation
"""

augmented_train_df = pd.read_parquet(save_path + '/'+ 'augmented_train_df.gzip')

ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)

augmented_train_df['augmented_text_str'] = augmented_train_df['augmented_text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))
ctf_fit = ctf.fit(augmented_train_df['augmented_text_str'])

ctf_transform = ctf.transform(augmented_train_df['augmented_text_str'])

voc_ctf = ctf.get_feature_names_out()
voc_ctf_df = pd.DataFrame(ctf_transform.todense(), columns=voc_ctf)
print(voc_ctf_df.shape)
voc_ctf_df.head()

opt_perplexity_ctf = find_optimum_perplexity(voc_ctf_df)

y_cat_num = train_df['real_clusters']
# ARI_ctf, X_tsne_ctf, labels_ctf, fit_time_ctf  = calc_ari(ctf_transform, opt_perplexity_ctf) # ARI: 0.4532, Time: 6.02s for both random and pca inits
ARI_ctf, X_tsne_ctf, X_umap_ctf, labels_ctf, fit_time_ctf  = calc_ari(voc_ctf_df, opt_perplexity_ctf)
# model_results.append({'Model': 'Tf-idf', 'ARI': ARI_ctf, 'Fitting Time (s)': fit_time_ctf})
print('ARI :', ARI_ctf, 'Fitting Time (s):', fit_time_ctf)

voc_ctf_df.to_csv(save_path + '/'+ 'text_features_train_aug.csv')



"""#### 1.8.1.1-b - Without data augmentation"""

ctf_no_aug = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)

train_df['text_str'] = train_df['corpus_bow_lem'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))
ctf_fit_no_aug = ctf_no_aug.fit(train_df['text_str'])

ctf_transform_no_aug = ctf_no_aug.transform(train_df['text_str'])

voc_ctf_no_aug = ctf_no_aug.get_feature_names_out()
voc_ctf_df_no_aug = pd.DataFrame(ctf_transform_no_aug.todense(), columns=voc_ctf_no_aug)
print(voc_ctf_df_no_aug.shape)
voc_ctf_df_no_aug.head()

opt_perplexity_ctf_no_aug = find_optimum_perplexity(voc_ctf_df_no_aug)

y_cat_num = train_df['real_clusters']
# ARI_ctf, X_tsne_ctf, labels_ctf, fit_time_ctf  = calc_ari(ctf_transform, opt_perplexity_ctf) # ARI: 0.4532, Time: 6.02s for both random and pca inits
ARI_ctf_no_aug, X_tsne_ctf_no_aug, X_umap_ctf_no_aug, labels_ctf_no_aug, fit_time_ctf_no_aug  = calc_ari(voc_ctf_df_no_aug, opt_perplexity_ctf_no_aug)
# model_results.append({'Model': 'Tf-idf', 'ARI': ARI_ctf, 'Fitting Time (s)': fit_time_ctf})
print('ARI :', ARI_ctf_no_aug, 'Fitting Time (s):', fit_time_ctf_no_aug)

voc_ctf_df_no_aug.to_csv(save_path + '/'+ 'text_features_train_no_aug.csv')



"""#### 1.8.1.2 - Validation text data

#### 1.8.1.2-a - With data augmentation
"""

# use vectorizer fitted on augmented training data to ensure same number of features
ctf_val_transform = ctf_fit.transform(val_df['corpus_bow_lem'])
voc_ctf_val = ctf_fit.get_feature_names_out() # Use ctf_fit, not just ctf
voc_ctf_val_df = pd.DataFrame(ctf_val_transform.todense(), columns=voc_ctf_val, index=val_df.index)
print(voc_ctf_val_df.shape)
voc_ctf_val_df.head()

voc_ctf_val_df.to_csv(save_path + '/'+'text_features_val_aug.csv')



"""#### 1.8.1.2-b - Without data augmentation"""

# use vectorizer fitted on training data to ensure same number of features
ctf_val_transform_no_aug = ctf_fit_no_aug.transform(val_df['corpus_bow_lem'])
voc_ctf_val_no_aug = ctf_fit_no_aug.get_feature_names_out() # Use ctf_fit, not just ctf
voc_ctf_val_df_no_aug = pd.DataFrame(ctf_val_transform_no_aug.todense(), columns=voc_ctf_val_no_aug, index=val_df.index)
print(voc_ctf_val_df_no_aug.shape)
voc_ctf_val_df_no_aug.head()

voc_ctf_val_df_no_aug.to_csv(save_path + '/'+'text_features_val_no_aug.csv')



"""#### 1.8.1.3 - Test text data

#### 1.8.1.3-a - With data augmentation
"""

# use vectorizer fitted on augmented training data to ensure same number of features
ctf_test_transform = ctf_fit.transform(test_df['corpus_bow_lem'])
voc_ctf_test = ctf_fit.get_feature_names_out()
voc_ctf_test_df = pd.DataFrame(ctf_test_transform.todense(), columns=voc_ctf_test, index=test_df.index)
print(voc_ctf_test_df.shape)
voc_ctf_test_df.head()

voc_ctf_test_df.to_csv(save_path + '/'+'text_features_test_aug.csv')



"""#### 1.8.1.3-a - Without data augmentation"""

# use vectorizer fitted on augmented training data to ensure same number of features
ctf_test_transform_no_aug = ctf_fit_no_aug.transform(test_df['corpus_bow_lem'])
voc_ctf_test_no_aug = ctf_fit_no_aug.get_feature_names_out()
voc_ctf_test_df_no_aug = pd.DataFrame(ctf_test_transform_no_aug.todense(), columns=voc_ctf_test_no_aug, index=test_df.index)
print(voc_ctf_test_df_no_aug.shape)
voc_ctf_test_df_no_aug.head()

voc_ctf_test_df_no_aug.to_csv(save_path + '/'+'text_features_test_no_aug.csv')



"""### 1.8.2 - Images - using ResNet50

#### 1.8.2.1 - Training image data

##### 1.8.2.1-a - Training image data with augmentation
"""

# Generate subsets only once - remove folder before re-generating
# save_image_subsets(image_path_aug, image_save_path + '/' + 'train_images_subsets_aug', val_test_df=train_df)

list_photos = [file for file in listdir(image_path_aug)]
print(len(list_photos))

# Load ResNet50 pretrained on ImageNet, exclude top classification layer, use global average pooling
base_model_50 = ResNet50(weights='imagenet', include_top=False, pooling='avg')

# The model outputs a 2048-dimensional feature vector per image
model_50 = base_model_50

# print(model_50.summary())

temps_3 = time.time()

images_features_50 = []
i=0
for image_num in range(len(list_photos)) :
    if i%100 == 0 :
      print(f'progress : {i / len(list_photos) * 100:.2f} %')
    i +=1
    image = load_img(image_path_aug+'/'+list_photos[image_num], target_size=(224, 224))
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)

    image = preprocess_input(image)
    images_features_50.append(model_50.predict(image, verbose=0)[0])

    duration_3 = time.time() - temps_3
print("Features creation time with RN50 : ", "%15.2f" % duration_3, "secs")

images_features_50 = np.asarray(images_features_50)
images_features_50.shape

images_features_50 = pd.DataFrame(images_features_50)
images_features_50.to_csv(save_path +'/'+'image_features_train_aug.csv', index=False)



"""##### 1.8.2.1-b - Training image data without augmentation"""

# Generate subsets only once - remove folder before re-generating
# save_image_subsets(image_path, image_save_path + '/' + 'train_images_subsets', val_test_df=train_df)

train_df = pd.read_parquet(save_path + '/'+ 'train_df.gzip')
save_image_set(image_path, image_save_path + '/' + 'train_images', val_test_df=train_df)

train_list_photos = [file for file in listdir(image_save_path + '/' + 'train_images')]
print(len(train_list_photos))

# Load ResNet50 pretrained on ImageNet, exclude top classification layer, use global average pooling
base_model_50_train = ResNet50(weights='imagenet', include_top=False, pooling='avg')

# The model outputs a 2048-dimensional feature vector per image
model_50_train = base_model_50_train

# print(model_50_train.summary())

temps_9 = time.time()

images_features_50_train = []
i=0
for image_num in range(len(train_list_photos)) :
    if i%25 == 0 :
      print(f'progress : {i / len(train_list_photos) * 100:.2f} %')
    i +=1
    image_train = load_img(image_save_path + '/' + 'train_images'+'/'+train_list_photos[image_num], target_size=(224, 224))
    image_train = img_to_array(image_train)
    image_train = np.expand_dims(image_train, axis=0)

    image_train = preprocess_input(image_train)
    images_features_50_train.append(model_50_train.predict(image_train, verbose=0)[0])

    duration_9 = time.time() - temps_9
print("train features creation time with RN50 : ", "%15.2f" % duration_9, "secs")

images_features_50_train = np.asarray(images_features_50_train)
images_features_50_train.shape

image_features_train = pd.DataFrame(images_features_50_train)
image_features_train.to_csv(save_path +'/'+'image_features_train_no_aug.csv', index=False)



"""#### 1.8.2.2 - Validation image data"""

# Generate subsets only once - remove folder before re-generating
# save_image_subsets(save_path +'/' + 'val_images', save_path + '/' + 'val_images_subsets', val_test_df=val_df)

# val_df = pd.read_parquet(save_path + '/'+'val_df.gzip')
save_image_set(image_path, image_save_path + '/' + 'val_images', val_test_df=val_df)

val_list_photos = [file for file in listdir(image_save_path + '/' + 'val_images')]
print(len(val_list_photos))

# Load ResNet50 pretrained on ImageNet, exclude top classification layer, use global average pooling
base_model_50_val = ResNet50(weights='imagenet', include_top=False, pooling='avg')

# The model outputs a 2048-dimensional feature vector per image
model_50_val = base_model_50_val

# print(model_50.summary())

temps_5 = time.time()

images_features_50_val = []
i=0
for image_num in range(len(val_list_photos)) :
    if i%25 == 0 :
      print(f'progress : {i / len(val_list_photos) * 100:.2f} %')
    i +=1
    image_val = load_img(image_save_path + '/' + 'val_images'+'/'+val_list_photos[image_num], target_size=(224, 224))
    image_val = img_to_array(image_val)
    image_val = np.expand_dims(image_val, axis=0)

    image_val = preprocess_input(image_val)
    images_features_50_val.append(model_50_val.predict(image_val, verbose=0)[0])

    duration_5 = time.time() - temps_5
print("Val features creation time with RN50 : ", "%15.2f" % duration_5, "secs")

images_features_50_val = np.asarray(images_features_50_val)
images_features_50_val.shape

image_features_val = pd.DataFrame(images_features_50_val)
image_features_val.to_csv(save_path +'/'+'image_features_val.csv', index=False)



"""#### 1.8.2.3 - Test image data"""

# Generate subsets only once - remove folder before re-generating
# save_image_subsets(save_path +'/' + 'test_images', save_path + '/' + 'test_images_subsets', val_test_df=test_df)

test_df = pd.read_parquet(save_path + '/'+'test_df.gzip')
save_image_set(image_path, image_save_path + '/' + 'test_images', val_test_df=test_df)

test_list_photos = [file for file in listdir(image_save_path + '/' + 'test_images')]
print(len(test_list_photos))

# Load ResNet50 pretrained on ImageNet, exclude top classification layer, use global average pooling
base_model_50_test = ResNet50(weights='imagenet', include_top=False, pooling='avg')

# The model outputs a 2048-dimensional feature vector per image
model_50_test = base_model_50_test

# print(model_50.summary())

temps_7 = time.time()

images_features_50_test = []
i=0
for image_num in range(len(test_list_photos)) :
    if i%25 == 0 :
      print(f'progress : {i / len(test_list_photos) * 100:.2f} %')
    i +=1
    image_test = load_img(image_save_path + '/' + 'test_images'+'/'+test_list_photos[image_num], target_size=(224, 224))
    image_test = img_to_array(image_test)
    image_test = np.expand_dims(image_test, axis=0)

    image_test = preprocess_input(image_test)
    images_features_50_test.append(model_50_test.predict(image_test, verbose=0)[0])

    duration_7 = time.time() - temps_7
print("Test features creation time with RN50 : ", "%15.2f" % duration_7, "secs")

images_features_50_test = np.asarray(images_features_50_test)
images_features_50_test.shape

image_features_test = pd.DataFrame(images_features_50_test)
image_features_test.to_csv(save_path +'/'+'image_features_test.csv', index=False)



"""### 1.8.3 - Merge text and image features

#### 1.8.3.1 - Training data

#### 1.8.3.1-a - With data augmentation
"""

train_df = pd.read_parquet(save_path + '/'+'train_df.gzip')
y_train = train_df['real_clusters']

text_features_aug = pd.read_csv(save_path +'/' + 'text_features_train_aug.csv').drop(columns=['Unnamed: 0'])
print(text_features_aug.shape)
text_features_aug.head(1)

image_features_aug = pd.read_csv(save_path +'/'+'image_features_train_aug.csv')
print(image_features_aug.shape)
image_features_aug.head(1)

text_features_aug = text_features_aug.reset_index(drop=True)
image_features_aug = image_features_aug.reset_index(drop=True)

X_train_aug = pd.concat([text_features_aug, image_features_aug], axis=1)
print(X_train_aug.shape)
X_train_aug.to_csv(save_path +'/' + 'X_train_aug.csv', index=False)
X_train_aug.head(1)





"""#### 1.8.3.1-b - Without data augmentation"""

train_df = pd.read_parquet(save_path + '/'+'train_df.gzip')
y_train = train_df['real_clusters']

text_features_no_aug = pd.read_csv(save_path +'/' + 'text_features_train_no_aug.csv').drop(columns=['Unnamed: 0'])
print(text_features_no_aug.shape)
text_features_no_aug.head(1)

image_features_train_no_aug = pd.read_csv(save_path +'/'+'image_features_train_no_aug.csv')
print(image_features_train_no_aug.shape)
image_features_train_no_aug.head(1)

text_features_no_aug = text_features_no_aug.reset_index(drop=True)
image_features_no_aug = image_features_train_no_aug.reset_index(drop=True)

X_train_no_aug = pd.concat([text_features_no_aug, image_features_no_aug], axis=1)
print(X_train_no_aug.shape)
X_train_no_aug.to_csv(save_path +'/' + 'X_train_no_aug.csv', index=False)
X_train_no_aug.head(1)

train_df = pd.read_parquet(save_path + '/'+'train_df.gzip')
y_train = train_df['real_clusters']



"""#### 1.8.3.2 - Validation data

#### 1.8.3.2-a - With data augmentation
"""

val_df=pd.read_parquet(save_path + '/'+'val_df.gzip')
y_val = val_df['real_clusters']

val_text_features_aug = pd.read_csv(save_path +'/' + 'text_features_val_aug.csv').drop(columns=['Unnamed: 0'])
print(val_text_features_aug.shape)
val_text_features_aug.head(1)

image_features_val_aug = pd.read_csv(save_path +'/' + 'image_features_val.csv')
print(image_features_val.shape)
image_features_val.head(1)

X_val_aug = pd.concat([val_text_features_aug, image_features_val], axis=1)
print(X_val_aug.shape)
X_val_aug.to_csv(save_path +'/' + 'X_val_aug.csv', index=False)



"""#### 1.8.3.2-b - Without data augmentation"""

val_text_features_no_aug = pd.read_csv(save_path +'/' + 'text_features_val_no_aug.csv').drop(columns=['Unnamed: 0'])
print(val_text_features_no_aug.shape)
val_text_features_no_aug.head(1)

image_features_val_aug = pd.read_csv(save_path +'/' + 'image_features_val.csv')
print(image_features_val.shape)
image_features_val.head(1)

X_val_no_aug = pd.concat([val_text_features_no_aug, image_features_val], axis=1)
print(X_val_no_aug.shape)
X_val_no_aug.to_csv(save_path +'/' + 'X_val_no_aug.csv', index=False)



"""#### 1.8.3.3 - Test data

#### 1.8.3.3-a - With data augmentation
"""

test_df=pd.read_parquet(save_path + '/'+'test_df.gzip')
y_test = test_df['real_clusters']

test_text_features_aug = pd.read_csv(save_path +'/' + 'text_features_test_aug.csv').drop(columns=['Unnamed: 0'])
print(test_text_features_aug.shape)
test_text_features_aug.head(1)

image_features_test = pd.read_csv(save_path +'/' + 'image_features_test.csv')
print(image_features_test.shape)
image_features_test.head(1)

X_test_aug = pd.concat([test_text_features_aug, image_features_test], axis=1)
print(X_test_aug.shape)
X_test_aug.to_csv(save_path +'/' + 'X_test_aug.csv', index=False)



"""#### 1.8.3.3-b - Without data augmentation"""

test_text_features_no_aug = pd.read_csv(save_path +'/' + 'text_features_test_no_aug.csv').drop(columns=['Unnamed: 0'])
print(test_text_features_no_aug.shape)
test_text_features_no_aug.head(1)

image_features_test = pd.read_csv(save_path +'/' + 'image_features_test.csv')
print(image_features_test.shape)
image_features_test.head(1)

X_test_no_aug = pd.concat([test_text_features_no_aug, image_features_test], axis=1)
print(X_test_no_aug.shape)
X_test_no_aug.to_csv(save_path +'/' + 'X_test_no_aug.csv', index=False)



"""# 2 - Supervised classification - image data only

## 2.1 - Instanciate model
"""

def init_model():
    # Initialize ResNet50 base model
    model_init = ResNet50(include_top=False,
                         weights="imagenet",
                         input_shape=(224, 224, 3))  # Changed to ResNet50

    # Freeze all layers in base model
    for layer in model_init.layers:
        layer.trainable = False

    # Add custom top layers
    x = model_init.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(7, activation='softmax')(x)  # Keep final layer

    # Create full model
    model = Model(inputs=model_init.input, outputs=predictions)

    # Compile model (keeping original optimizer/loss)
    model.compile(loss="categorical_crossentropy",
                 optimizer='rmsprop',
                 metrics=['accuracy', tf.keras.metrics.F1Score(name='f1_macro', average='macro')])

    # print(model.summary())
    return model

def init_model_aug():
    # Data augmentation
    data_augmentation = Sequential([
        RandomFlip("horizontal", input_shape=(224, 224, 3)),
        RandomRotation(0.1),
        RandomZoom(0.1),
        ])

    # Initialize ResNet50 base model
    model_init = ResNet50(include_top=False,
                         weights="imagenet",
                         input_shape=(224, 224, 3))

    # Freeze all layers in base model
    for layer in model_init.layers:
        layer.trainable = False

    # Add custom top layers, including data augmentation
    x = model_init.input
    x = data_augmentation(x)
    x = model_init(x)
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(7, activation='softmax')(x)

    # Create full model
    model = Model(inputs=model_init.input, outputs=predictions)

    # Compile model
    model.compile(loss="categorical_crossentropy",
                  optimizer='rmsprop',
                  metrics=['accuracy', tf.keras.metrics.F1Score(name='f1_macro', average='macro')])

    # print(model.summary())
    return model

def train_and_evaluate_model(model_name, model, epochs, dataset_train, dataset_val, callbacks=None, verbose=0):
    if callbacks is None:
        callbacks = []  # Default is empty list

    if not any(isinstance(cb, EarlyStopping) for cb in callbacks):
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        callbacks.append(early_stopping)
    else:
        early_stopping = [cb for cb in callbacks if isinstance(cb, EarlyStopping)][0]

    start_time = time.time()
    history = model.fit(dataset_train, epochs=epochs, validation_data=dataset_val, callbacks=callbacks, verbose=verbose)
    end_time = time.time()
    fit_time = end_time - start_time

    early_stopping_epoch = early_stopping.stopped_epoch + 1 if early_stopping.stopped_epoch is not None else len(history.history['loss'])

    y_true_val = []
    for _, labels in dataset_val:
        y_true_val.extend(np.argmax(labels.numpy(), axis=1))

    y_true_val = np.array(y_true_val)


    y_pred_val_probs = model.predict(dataset_val, verbose=verbose)

    y_pred_val = np.argmax(y_pred_val_probs, axis=1)


    val_accuracy = accuracy_score(y_true_val, y_pred_val)
    val_precision_macro = precision_score(y_true_val, y_pred_val, average='macro', zero_division=0)
    val_recall_macro = recall_score(y_true_val, y_pred_val, average='macro', zero_division=0)
    val_f1_macro_sklearn = f1_score(y_true_val, y_pred_val, average='macro', zero_division=0) # Recalculate with sklearn for consistency
    val_f2_macro = fbeta_score(y_true_val, y_pred_val, beta=2, average='macro', zero_division=0)


    train_acc_hist = history.history['accuracy'][-1]
    val_acc_hist = history.history['val_accuracy'][-1]
    train_f1_hist = history.history['f1_macro'][-1]
    val_f1_hist = history.history['val_f1_macro'][-1]


    model_data = {
        'model_name': model_name,
        'epochs_run': len(history.history['loss']),
        'early_stopping_epoch': early_stopping_epoch,
        'fit_time': fit_time,


        'Train Accuracy (Hist)': train_acc_hist,
        'Val Accuracy (Hist)': val_acc_hist,
        'Train f1_macro (Hist)': train_f1_hist,
        'Val f1_macro (Hist)': val_f1_hist,

        'Val Accuracy (Sklearn)': val_accuracy,
        'Val Precision (Sklearn)': val_precision_macro,
        'Val Recall (Sklearn)': val_recall_macro,
        'Val f1_macro (Sklearn)': val_f1_macro_sklearn,
        'Val f2_macro (Sklearn)': val_f2_macro
    }

    return model_data, history



"""## 2.2 - Import image sets"""

batch_size = 32

def dataset_fct(path, validation_split=0, data_type=None) :
    dataset = tf.keras.utils.image_dataset_from_directory(
                    path, labels='inferred', label_mode='categorical',
                    class_names=None, batch_size=batch_size, image_size=(224, 224), shuffle=False,
                    seed=rs, validation_split=validation_split, subset=data_type
                    )
    return dataset

dataset_train_aug = dataset_fct(path=save_path + '/' + 'train_images_subsets_aug', validation_split=0, data_type=None)

dataset_train = dataset_fct(path=save_path + '/' + 'train_images_subsets', validation_split=0, data_type=None)

dataset_val = dataset_fct(path=save_path + '/' + 'val_images_subsets', validation_split=0, data_type=None)

dataset_test = dataset_fct(path=save_path + '/' + 'test_images_subsets', validation_split=0, data_type=None)

dataset_train_val = dataset_fct(path=save_path + '/' + 'train_val_images_subsets', validation_split=0, data_type=None)

dataset_train_val_aug = dataset_fct(path=save_path + '/' + 'train_val_images_subsets_aug', validation_split=0, data_type=None)

dataset_train_val_ALL = dataset_fct(path=save_path + '/' + 'train_val_images_subsets_ALL', validation_split=0, data_type=None)



"""## 2.3 - Modelisation with external image augmentation"""

# Create model
with tf.device('/gpu:0'):
    model_1 = init_model()

# Create callbacks
model_1_save_path = "./model_1_best_weights.h5"
checkpoint_1 = ModelCheckpoint(model_1_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_1 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_1 = [checkpoint_1, es_1]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on augmented images & evaluate
    model_1_data, history_1 = train_and_evaluate_model('ResNet50 External Data Augmentation', model_1, epochs=50,
                                                     dataset_train=dataset_train_aug, dataset_val=dataset_val,
                                                     callbacks=callbacks_list_1)
    models_data.append(model_1_data)
    histories['model_1'] = history_1

show_history(histories['model_1'])
plot_history(histories['model_1'], path="save_path + '/' + ResNet50_augmented_data.png")
plt.close()



"""## 2.4 - Modelisation with integrated image augmentation"""

# Create model
with tf.device('/gpu:0'):
    model_2 = init_model_aug()

# Create callbacks
model_2_save_path = "./model_2_best_weights.h5"
checkpoint_2 = ModelCheckpoint(model_2_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_2 = [checkpoint_2, es_2]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on original images & evaluate
    model_2_data, history_2 = train_and_evaluate_model('ResNet50 Integrated Data Augmentation', model_2, epochs=50,
                                                     dataset_train=dataset_train, dataset_val=dataset_val,
                                                     callbacks=callbacks_list_2)
    models_data.append(model_2_data)
    histories['model_2'] = history_2

show_history(histories['model_2'])
plot_history(histories['model_2'], path="save_path + '/' + ResNet50_integrated_augmentation.png")
plt.close()



"""## 2.5 - Modelisation without image augmentation"""

# Create model
with tf.device('/gpu:0'):
    model_3 = init_model()

model_3_save_path = "./model_3_best_weights.h5"
checkpoint_3 = ModelCheckpoint(model_3_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_3 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_3 = [checkpoint_3, es_3]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on original images & evaluate
    model_3_data, history_3 = train_and_evaluate_model('ResNet50 Original Data', model_3, epochs=50,
                                                     dataset_train=dataset_train, dataset_val=dataset_val,
                                                     callbacks=callbacks_list_3)
    models_data.append(model_3_data)
    histories['model_3'] = history_3

show_history(histories['model_3'])
plot_history(histories['model_3'], path="save_path + '/' + ResNet50_original_data.png")
plt.close()



"""## 2.6 - Modelisation on Train + Val sets"""

# Create model
with tf.device('/gpu:0'):
    model_4 = init_model()

model_4_save_path = "./model_4_best_weights.h5"
checkpoint_4 = ModelCheckpoint(model_4_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_4 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_4 = [checkpoint_4, es_4]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on original images & evaluate
    model_4_data, history_4 = train_and_evaluate_model('ResNet50 Original Data Train + Val', model_4, epochs=50,
                                                     dataset_train=dataset_train_val, dataset_val=dataset_test,
                                                     callbacks=callbacks_list_4)
    models_data.append(model_4_data)
    histories['model_4'] = history_4

show_history(histories['model_4'])
plot_history(histories['model_4'], path="save_path + '/' + ResNet50_original_train&val_data.png")
plt.close()

y_pred_test_probs = model_4.predict(dataset_test)
y_pred_test = np.argmax(y_pred_test_probs, axis=1)

y_true_test = []
for _, labels in dataset_test:
  y_true_test.extend(np.argmax(labels.numpy(), axis=1))
y_true_test = np.array(y_true_test)

print(classification_report(y_true_test, y_pred_test))

cm = confusion_matrix(y_true_test, y_pred_test)

class_names = dataset_test.class_names

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model 4 (ResNet50 on Train + Val)')
plt.savefig(save_path + '/' + 'confusion_matrix_model_4.png')
plt.show()

"""## 2.7 - Modelisation on train + val sets augmented"""

# Create model
with tf.device('/gpu:0'):
    model_5 = init_model()

model_5_save_path = "./model_5_best_weights.h5"
checkpoint_5 = ModelCheckpoint(model_5_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_5 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_5 = [checkpoint_5, es_5]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on original images & evaluate
    model_5_data, history_5 = train_and_evaluate_model('ResNet50 Augmented Data Train + Val', model_5, epochs=50,
                                                     dataset_train=dataset_train_val_aug, dataset_val=dataset_test,
                                                     callbacks=callbacks_list_5)
    models_data.append(model_5_data)
    histories['model_5'] = history_5

show_history(histories['model_5'])
plot_history(histories['model_5'], path="save_path + '/' + ResNet50_aug_train&val_data.png")
plt.close()



"""## 2.8 - Modelisation on train + val sets & their augmentations"""

# Create model
with tf.device('/gpu:0'):
    model_6 = init_model()

model_6_save_path = "./model_6_best_weights.h5"
checkpoint_6 = ModelCheckpoint(model_6_save_path, monitor='val_loss', verbose=1,
                             save_best_only=True, mode='min')
es_6 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
callbacks_list_6 = [checkpoint_6, es_6]

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

with tf.device('/gpu:0'):
    # Train model on original images & evaluate
    model_6_data, history_6 = train_and_evaluate_model('ResNet50 Original + Augmented Data Train & Val', model_6, epochs=50,
                                                     dataset_train=dataset_train_val_ALL, dataset_val=dataset_test,
                                                     callbacks=callbacks_list_6)
    models_data.append(model_6_data)
    histories['model_6'] = history_6

show_history(histories['model_6'])
plot_history(histories['model_6'], path="save_path + '/' + ResNet50_aug&orig_train&val_data.png")
plt.close()



"""## 2.9 - Store model comparison metrics"""

# Create the DataFrame
models_data = pd.DataFrame(models_data)
models_data

models_data.to_csv(save_path + '/' + 'models_data.csv', index=False)



"""# 3 - Supervised classification using text and image features

## 3.1 - Define metrics, pipelines, pre-processing steps & hyperparameter grids
"""

# Define performance metrics
scorers = {'f1_macro':  make_scorer(f1_score, average='macro'),
           'f2_macro': make_scorer(fbeta_score, beta=2, average='macro'),
           'accuracy': make_scorer(accuracy_score),
           'precision': make_scorer(precision_score, average='macro'),
           'recall': make_scorer(recall_score, average='macro')
           }

# Split numerical and categorical columns and list column names
num_X_no_aug = X_train_no_aug.select_dtypes(include='number').columns.tolist()
# check
print(len(num_X_no_aug))

num_X_aug = X_train_aug.select_dtypes(include='number').columns.tolist()

# check
print(len(num_X_aug))

num_transfo_ens = Pipeline(steps=[('scaler', StandardScaler())])
preproc_ens_no_aug = ColumnTransformer(transformers=[('num', num_transfo_ens, num_X_no_aug)], remainder='passthrough')
preproc_ens_aug = ColumnTransformer(transformers=[('num', num_transfo_ens, num_X_aug)], remainder='passthrough')

# initialise models
models = [
    ('Random Forest Regression', RandomForestClassifier(verbose=0, n_jobs=-1, random_state=rs)), # Baseline model
    ('LightGBM', LGBMClassifier(verbose=-1, n_jobs=-1, random_state=rs)),
    ('XGBoost', XGBClassifier(verbose=0, n_jobs=-1, random_state=rs))
]

# create list of ensemble models
ens_models = ['Random Forest Regression', 'LightGBM', 'XGBoost']

# define hyperparameter grids
param_grids = [
    # for Random Forest Regression
    {'model__n_estimators': [400, 800],
      'model__max_depth' : [10, 20],
      'model__max_features' : ["sqrt"],
      'model__min_samples_split' : [2]},
    # for LightGBM
    {'model__max_depth': [6, 8], # decrease for faster convergence
     'model__num_leaves':[60, 200], # should be smaller than 2^(max_depth) - decrease for faster convergence
     'model__min_data_in_leaf':[200, 400], # very important parameter to prevent over-fitting in a leaf-wise tree, hundreds or thousands is enough for a large dataset
     'model__learning_rate': [0.1, 0.2], # increase if decreasing num_iterations
     'model__min_gain_to_split': [0.001]}, # default is 0 (no gain too small) - increase to reduce training time
    # for XGBoost
    {'model__booster':['gbtree'], # gblinear also available
     'model__device':['cuda'], # use cuda for GPU acceleration, otherwise use 'cpu'
     'model__learning_rate': [0.1, 0.2], # see also eta - default is 0.3
     'model__min_split_loss': [0.1], # see also gamma - default is 0
     'model__min_child_weight':[5],
     'model__max_depth': [6, 8], # default is 6
     'model__max_delta_step' :[5, 8],
     'model__n_estimators':[400, 800],
     'model__tree_method':['gpu_hist'], # default is 'auto'
     'model__subsample': [0.75], # set >=0.5 for good results with uniform sampling method
     'model__colsample_bytree' : [0.75], # default is 1 - decrease to control overfitting
     'model__colsample_bylevel' : [0.75], # default is 1 - decrease to control overfitting
     'model__colsample_bynode' : [0.75], # default is 1 - decrease to control overfitting
     'model__sampling_method':['gradient_based'] # gradient-based sampling works only with tree_method=gpu_hist and device=cuda
    }]



"""## 3.2 - Without data augmentation of training set"""

# fit models using GridSearchCV

# initialize empty list to store models evaluation metrics
model_metrics_no_aug = []

# iterate over models and respective parameter grid
for (name, model), param_grid in zip(models, param_grids):
    # create pipeline with relevant preprocessor and model
    if name in ens_models:
        pipe = Pipeline(steps=[('preprocessor', preproc_ens_no_aug), ('model', model)])
    else:
        # pipe = Pipeline(steps=[('preprocessor', preproc_lin), ('model', model)])
        print('Model not in list - check ensemble_models')

    k_values = [5]

    for k in k_values:
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rs)
        spl = skf.split(X_train_no_aug, y_train)

        # Perform GridSearchCV on the pipeline with the current hyperparameter grid
        grid_search = GridSearchCV(pipe, param_grid, cv=spl, scoring=scorers, refit='f1_macro', error_score='raise', verbose=True)

        # Fit the GridSearchCV object on the training data
        train_start = timer()
        X_train_no_aug.columns = X_train_no_aug.columns.astype(str)
        X_val_no_aug.columns = X_val_no_aug.columns.astype(str)
        grid_search.fit(X_train_no_aug, y_train.values)
        train_end = timer()
        train_time = train_end - train_start

        # Get the best hyperparameters and best training score
        best_params = grid_search.best_params_
        best_score = grid_search.best_score_

        # evaluate model on validation set & get score
        val_start = timer()
        val_score = grid_search.score(X_val_no_aug, y_val) # uses score defined by scoring or best_estimator_.score
        y_pred_no_aug = grid_search.predict(X_val_no_aug)
        val_end = timer()
        val_time = val_end - val_start

        # Extract coefficients from the best estimator
        best_model = grid_search.best_estimator_

    # append results to results list
    model_metrics_no_aug.append({
        'Model': model,
        'Training set':'X_train_no_aug',
        'Best parameters': best_params,
        'Train f1_macro': best_score,
        'Train fit time': train_time,
        'Val f1_macro': val_score,
        'Val f2_macro': fbeta_score(y_val, y_pred_no_aug, beta=2, average='macro'),
        'Val Accuracy': accuracy_score(y_val, y_pred_no_aug),
        'Val Precision': precision_score(y_val, y_pred_no_aug, average='macro'),
        'Val Recall': recall_score(y_val, y_pred_no_aug, average='macro'),
        'Strat. K-Fold' : skf
    })

# store model metrics & coefs in dataframes
model_no_aug_compare = pd.DataFrame(model_metrics_no_aug)

# display dataframe
model_no_aug_compare

# Generate timestamp and save df
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
filename = f"model_no_aug_compare_{timestamp}.csv"
model_no_aug_compare.to_csv(save_path + '/' + filename, index=False)

pd.read_csv(save_path + '/' + 'model_no_aug_compare_20250515_205900.csv')



"""## 3.3 - With data augmentation of training set"""

# fit models using GridSearchCV

# initialize empty list to store models evaluation metrics
model_metrics_aug = []

# iterate over models and respective parameter grid
for (name, model), param_grid in zip(models, param_grids):
    # create pipeline with relevant preprocessor and model
    if name in ens_models:
        pipe = Pipeline(steps=[('preprocessor', preproc_ens_aug), ('model', model)])
    else:
        # pipe = Pipeline(steps=[('preprocessor', preproc_lin), ('model', model)])
        print('Model not in list - check ensemble_models')

    k_values = [5]

    for k in k_values:
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rs)
        spl = skf.split(X_train_aug, y_train)

        # Perform GridSearchCV on the pipeline with the current hyperparameter grid
        grid_search = GridSearchCV(pipe, param_grid, cv=spl, scoring=scorers, refit='f1_macro', error_score='raise', verbose=True)

        # Fit the GridSearchCV object on the training data
        train_start = timer()
        X_train_aug.columns = X_train_aug.columns.astype(str)
        X_val_aug.columns = X_val_aug.columns.astype(str)
        grid_search.fit(X_train_aug, y_train.values)
        train_end = timer()
        train_time = train_end - train_start

        # Get the best hyperparameters and best training score
        best_params = grid_search.best_params_
        best_score = grid_search.best_score_

        # evaluate model on validation set & get score
        val_start = timer()
        val_score = grid_search.score(X_val_aug, y_val) # uses score defined by scoring or best_estimator_.score
        y_pred_aug = grid_search.predict(X_val_aug)
        val_end = timer()
        val_time = val_end - val_start

        # Extract coefficients from the best estimator
        best_model = grid_search.best_estimator_

    # append results to results list
    model_metrics_aug.append({
        'Model': model,
        'Training set':'X_train_aug',
        'Best parameters': best_params,
        'Train f1_macro': best_score,
        'Train fit time': train_time,
        'Val f1_macro': val_score,
        'Val f2_macro': fbeta_score(y_val, y_pred_aug, beta=2, average='macro'),
        'Val Accuracy': accuracy_score(y_val, y_pred_aug),
        'Val Precision': precision_score(y_val, y_pred_aug, average='macro'),
        'Val Recall': recall_score(y_val, y_pred_aug, average='macro'),
        'Strat. K-Fold' : skf
    })

# store model metrics & coefs in dataframes
model_aug_compare = pd.DataFrame(model_metrics_aug)

# display dataframe
model_aug_compare

# Generate timestamp and save df
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
filename = f"model_aug_compare_{timestamp}.csv"
model_aug_compare.to_csv(save_path + '/' + filename, index=False)

model_compare_all = pd.DataFrame(model_metrics_no_aug)
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
filename = f"model_compare_all_{timestamp}.csv"
model_compare_all.to_csv(save_path + '/' + filename, index=False)

pd.read_csv(save_path + '/' + 'model_compare_all_20250515_222941.csv')



"""## 3.4 - Retrain & evaluate best model on test data"""

# params & pipe from best model run
best_params = {'max_depth': 10,
               'max_features': 'sqrt',
               'min_samples_split': 2,
               'n_estimators': 800
              }

best_model = RandomForestClassifier(**best_params, verbose=0, random_state=rs)

timestamp_11 = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
best_model.fit(X_train_val_no_aug, y_train_val)

y_pred_best = best_model.predict(X_test_no_aug)

final_scores = {'f1_score': f1_score(y_test, y_pred_best, average='macro'),
                'f2_score': fbeta_score(y_test, y_pred_best, beta=2, average='macro') ,
                'accuracy': accuracy_score(y_test, y_pred_best),
                'precision': precision_score(y_test, y_pred_best, average='macro'),
                'recall': recall_score(y_test, y_pred_best, average='macro')
                }
final_scores

cats = pd.read_csv(save_path + '/' + 'category_mapping.csv')
cats

cats['class_names'] = cats['product_category'].astype(str) + ' - ' + cats['category_numeric'].astype(str)

cats

# plot confusion matrix
conf_mat_final = confusion_matrix(y_test, y_pred_best)
# class_names = cats['class_names']
class_names = cats['category_numeric']
plt.figure()
plot_confusion_matrix(conf_mat_final , classes=class_names, title='Confusion matrix - Final Model')
plt.savefig(save_path + '/' + 'Confusion matrix - Final Model.png', bbox_inches='tight')
plt.show()

"""**Class 6 (Watches) perfectly predicted, with class 4 (Home Furnishing) a close second, followed by class 2 (Computers).**"""



"""# !!! RUN NOTEBOOK BELOW THIS LINE ONLY !!!

# 4 - Supervised image classification - Transfer Learning  with MobileViTv2
"""

# run cell below first when restarting runtime in Google Colab
!pip install nlpaug plot_keras_history timm --quiet

# utilities
import sys
import datetime
from datetime import datetime
import random
import time
import logging
logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere
import os
import shutil
os.environ["TF_KERAS"]='1'
os.environ["TF_XLA_FLAGS"] = "--tf_xla_enable_xla_devices=false"
os.environ["OMP_NUM_THREADS"] = '1'  # needed to avoid memory leak warning with K-Means in Windows environment
from os import listdir
from glob import glob
from timeit import default_timer as timer

# data cleaning & processing
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

# dataviz
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
import plotly.express as px
from matplotlib.ticker import StrMethodFormatter
from matplotlib.ticker import FormatStrFormatter
from plot_keras_history import show_history, plot_history

# text processing
# import re
# import nltk
# from nltk.tokenize import word_tokenize, RegexpTokenizer
# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# nltk.download('punkt')
# nltk.download('punkt_tab')
# nltk.download('stopwords')
# nltk.download('wordnet')
# from nltk.corpus import stopwords
# from collections import defaultdict
# from nltk.stem import PorterStemmer, WordNetLemmatizer
# from collections import Counter
# from wordcloud import WordCloud

# text augmentation
# import nlpaug.augmenter.word as naw

# image processing
import cv2
from PIL import Image





# image augmentation
import albumentations as A
from albumentations.pytorch import ToTensorV2

# modelisation
from sklearn import cluster, metrics, manifold, decomposition
from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
# import lightgbm
# from lightgbm import LGBMClassifier
# import xgboost as xgb
# from xgboost import XGBClassifier
# import umap
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten, Dense, Dropout
from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# from tensorflow.keras.applications.vgg16 import VGG16
# from tensorflow.keras.applications.vgg16 import preprocess_input
# from tensorflow.keras.applications.vgg19 import VGG19
# from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical
# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
# from tensorflow.keras.applications import Xception, InceptionV3
# from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess
# from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess

# metrics
from sklearn import metrics
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, make_scorer, fbeta_score, precision_score, recall_score

# set dataframe display options
pd.set_option('max_colwidth', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: '%.4f' % x) # Suppress scientific notation and show only 4 decimals
# pd.set_option('display.float_format', lambda x: '%.f' % x) # Suppress scientific notation and show only integer part

# silence warnings after checking
import warnings
# pd.set_option('future.no_silent_downcasting', False) # introduced in pandas 2.0.0., this notebook uses 1.4.4
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=UserWarning)
# warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning) # introduced in pandas 2.0.0., this notebook uses 1.4.4
# from PIL import ImageDecompressionBombWarning
warnings.simplefilter('ignore', Image.DecompressionBombWarning)

# extract colors from logo for ppt slideshow
# banana = findColor('banana.png')
# print("banana hex :", banana)
banana = '#fcf7c9'

viridis_sample = ['#481567FF','#453781FF','#39568CFF','#2D708EFF','#238A8DFF','#20A387FF','#3CBB75FF', '#73D055FF','#B8DE29FF']

viridis_palette = ['#440154', '#481e70', '#443982', '#3a528b', '#30678d', '#287b8e', '#20908c', '#20a485', '#35b778', '#5ec961',
                   '#90d643', '#c7e01f', '#fde724']

sunset_palette = ["#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00", "#FF5722", "#FF3D00", "#FF2D00",
                  "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081", "#F50057", "#D5006D", "#C51162"]

palette = ['#440154', '#481e70', '#443982', '#3a528b', '#30678d', '#287b8e', '#20908c', '#20a485', '#35b778', '#5ec961',
           '#90d643', '#c7e01f', '#fde724', "#FFEB3B", "#FFDA44", "#FFC107", "#FFB300", "#FFA000", "#FF8F00", "#FF6F00",
           "#FF5722", "#FF3D00", "#FF2D00", "#E53935", "#D32F2F", "#C62828", "#B71C1C", "#FF5252", "#FF1744", "#FF4081",
           "#F50057", "#D5006D", "#C51162"]

from torch.utils.data import Dataset
import torch
from torchvision import transforms
from torch.utils.data import DataLoader
import timm
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report

import shap

# run this cell in Google Colab only
from google.colab import drive
drive.mount('/content/drive')
image_path = '/content/drive/My Drive/Colab Notebooks/OCDS_P6&P8/flipkart_images'
image_path_aug = '/content/drive/My Drive/Colab Notebooks/OCDS_P6&P8/augmented_images'
save_path = '/content/drive/My Drive/Colab Notebooks/OCDS_P6&P8'
image_save_path = save_path

train_df = pd.read_parquet(save_path + '/' + 'train_df.gzip')
print(train_df.shape)
# train_df.head(1)

val_df = pd.read_parquet(save_path + '/' + 'val_df.gzip')
print(val_df.shape)
# val_df.head(1)

test_df = pd.read_parquet(save_path + '/' + 'test_df.gzip')
print(test_df.shape)
# test_df.head(1)

list_labels=['0', '1', '2', '3', '4', '5', '6']



"""## 4.1 - Create image sets"""

class CustomImageDataset(Dataset):
    def __init__(self, df, image_col='image', label_col='real_clusters', root_dir=None, transform=None):
        self.df = df.reset_index(drop=True)
        self.image_col = image_col
        self.label_col = label_col
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_name = self.df.loc[idx, self.image_col]
        img_path = img_name if self.root_dir is None else f"{self.root_dir}/{img_name}"
        image = Image.open(img_path).convert('RGB')
        label = self.df.loc[idx, self.label_col]
        if self.transform:
            image = self.transform(image)
        label = torch.tensor(label, dtype=torch.long)  # <-- ensure label is LongTensor
        return image, label



"""## 4.2 - Define transforms"""

transform_train = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

transform_train_aug = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    # The two random transforms (RandomHorizontalFlip and RandomRotation) are classic data augmentation techniques.
    # They generate different versions of each image at each epoch, helping the model generalize better and reducing overfitting.
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])


transform_val_test = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

# Note : The normalization parameters are the per-channel mean and standard deviation values computed from the ImageNet dataset.
# They represent the average and spread of pixel values (scaled to ) for the red, green, and blue channels across the entire ImageNet training set.
# For transfer learning with ImageNet-pretrained models, it is standard to use these ImageNet values.



"""## 4.3 - Create data loaders"""

batch_size = 16

train_dataset = CustomImageDataset(train_df, root_dir=image_path, transform=transform_train)
val_dataset = CustomImageDataset(val_df, root_dir=image_path, transform=transform_val_test)
test_dataset = CustomImageDataset(test_df, root_dir=image_path, transform=transform_val_test)

train_val_dataset = CustomImageDataset(pd.concat([train_df, val_df]), root_dir=image_path, transform=transform_train)



train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

train_val_loader = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)





"""## 4.4 - Instanciate model"""

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# num_classes = len(train_df['real_clusters'].unique())
# model = timm.create_model('mobilevitv2_050', pretrained=True, num_classes=num_classes)
# model.to(device)
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=1e-4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(train_df['real_clusters'].unique())
model = timm.create_model('mobilevitv2_050', pretrained=True, num_classes=num_classes)

# Disable in-place ReLU operations
for module in model.modules():
    if isinstance(module, nn.ReLU):
        module.inplace = False  # Critical for SHAP compatibility

model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

print("In-place ReLUs remaining:",
      sum(1 for m in model.modules()
          if isinstance(m, nn.ReLU) and m.inplace))  # Should output 0

num_epochs = 50
patience = 5
best_val_acc = 0
epochs_no_improve = 0
best_model_state = None



"""## 4.5 - Training and evaluation functions"""

def train_one_epoch(model, dataloader, criterion, optimizer, verbose=False, print_every=10):
    model.train()
    running_loss = 0.0
    for batch_idx, (images, labels) in enumerate(dataloader):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if verbose and (batch_idx + 1) % print_every == 0:
            print(f"Batch {batch_idx+1}/{len(dataloader)} - Loss: {loss.item():.4f}")
    return running_loss / len(dataloader)

def evaluate(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())
    return all_labels, all_preds

def compute_accuracy(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total if total > 0 else 0



"""## 4.6 - Modelisation on training + valuation sets without data augmentation"""

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, verbose=True, print_every=10)
    train_acc = compute_accuracy(model, train_loader)
    val_labels, val_preds = evaluate(model, val_loader)
    val_report = classification_report(val_labels, val_preds, output_dict=True, zero_division=0)
    val_acc = val_report['accuracy']
    print(f"Epoch {epoch+1}/{num_epochs} - Train val loss: {train_loss:.4f} - Train val acc: {train_acc:.4f} - Test acc: {val_acc:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_no_improve = 0
        best_model_state = model.state_dict()  # Save the best model weights
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= patience:
        print(f"Early stopping triggered after {epoch+1} epochs. Best test acc: {best_val_acc:.4f}")
        break

# Optionally load the best model state before test evaluation
if best_model_state is not None:
    model.load_state_dict(best_model_state)

# prompt: add graphs showing accuracy, f1 macro and loss for train and val sets across epochs like the show_histories fuinction does in 2.6 above but for the MobileViTv2 model trained in 4.7

# Function to plot training history
def plot_history_mobilevit(history_dict, model_name, save_path=None):
    """
    Plots accuracy, f1_macro, and loss for train and validation sets.

    Args:
        history_dict (dict): Dictionary containing history metrics
                             (e.g., {'train_loss': [...], 'val_loss': [...], ...}).
        model_name (str): Name of the model for the plot title.
        save_path (str, optional): Path to save the plot. Defaults to None.
    """
    epochs = range(1, len(history_dict['train_loss']) + 1)

    plt.figure(figsize=(18, 5))

    # Plot Loss
    plt.subplot(1, 3, 1)
    plt.plot(epochs, history_dict['train_loss'], label='Training loss', color='blue')
    plt.plot(epochs, history_dict['val_loss'], label='Validation loss', color='orange')
    plt.title(f'{model_name} - Loss across Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(epochs, history_dict['train_accuracy'], label='Training accuracy', color='blue')
    plt.plot(epochs, history_dict['val_accuracy'],  label='Validation accuracy',color='orange')
    plt.title(f'{model_name} - Accuracy across Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot F1-macro
    plt.subplot(1, 3, 3)
    plt.plot(epochs, history_dict['train_f1_macro'], label='Training F1-macro', color='blue')
    plt.plot(epochs, history_dict['val_f1_macro'],  label='Validation F1-macro', color='orange')
    plt.title(f'{model_name} - F1-macro across Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('F1-macro')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)

    plt.show()

# Initialize dictionary to store history metrics for plotting
mobilevit_history = {
    'train_loss': [],
    'train_accuracy': [],
    'train_f1_macro': [],
    'val_loss': [],
    'val_accuracy': [],
    'val_f1_macro': []
}

# Re-run the training loop for MobileViTv2 with history tracking
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(train_df['real_clusters'].unique())
model = timm.create_model('mobilevitv2_050', pretrained=True, num_classes=num_classes)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
num_epochs = 50
patience = 5
best_val_acc = 0
epochs_no_improve = 0
best_model_state = None

print("Starting MobileViTv2 Training...")

for epoch in range(num_epochs):
    start_time = time.time()
    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, verbose=False) # Set verbose to False here to avoid excessive output per batch

    # Evaluate on training set to get metrics for plotting
    train_labels_eval, train_preds_eval = evaluate(model, train_loader)
    train_report_eval = classification_report(train_labels_eval, train_preds_eval, output_dict=True, zero_division=0)
    train_acc = train_report_eval['accuracy']
    train_f1_macro = train_report_eval['macro avg']['f1-score']

    # Evaluate on validation set
    val_labels, val_preds = evaluate(model, val_loader)
    val_report = classification_report(val_labels, val_preds, output_dict=True, zero_division=0)
    val_acc = val_report['accuracy']
    val_f1_macro = val_report['macro avg']['f1-score']

    # Calculate validation loss
    model.eval()
    val_running_loss = 0.0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item()
    val_loss = val_running_loss / len(val_loader)

    # Store metrics in history dictionary
    mobilevit_history['train_loss'].append(train_loss)
    mobilevit_history['train_accuracy'].append(train_acc)
    mobilevit_history['train_f1_macro'].append(train_f1_macro)
    mobilevit_history['val_loss'].append(val_loss)
    mobilevit_history['val_accuracy'].append(val_acc)
    mobilevit_history['val_f1_macro'].append(val_f1_macro)

    epoch_duration = time.time() - start_time

    print(f"Epoch {epoch+1}/{num_epochs} ({epoch_duration:.2f}s) - Train loss: {train_loss:.4f} - Train acc: {train_acc:.4f} - Train f1: {train_f1_macro:.4f} - Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f} - Val f1: {val_f1_macro:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_no_improve = 0
        # Save the best model state
        best_model_state = model.state_dict()
        print(f"Validation accuracy improved. Saving best model state with accuracy: {best_val_acc:.4f}")
    else:
        epochs_no_improve += 1
        print(f"Validation accuracy did not improve. Epochs without improvement: {epochs_no_improve}")

    if epochs_no_improve >= patience:
        print(f"Early stopping triggered after {epoch+1} epochs. Best validation accuracy: {best_val_acc:.4f}")
        break

print("MobileViTv2 Training Complete.")

# Load the best model state before final evaluation and plotting
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print("Loaded best model state based on validation accuracy.")

# Plot the training history
plot_history_mobilevit(mobilevit_history, 'MobileViTv2 Original Data (Train + Val)',
                       save_path=save_path + '/' + 'MobileViTv2_original_data_history.png')

# Final evaluation on the test set
print("\nEvaluating MobileViTv2 on the Test Set...")
test_labels, test_preds = evaluate(model, test_loader)
test_report = classification_report(test_labels, test_preds, output_dict=True, zero_division=0)

test_accuracy = test_report['accuracy']
test_f1_macro = test_report['macro avg']['f1-score']
test_precision_macro = test_report['macro avg']['precision']
test_recall_macro = test_report['macro avg']['recall']

print(f"\nMobileViTv2 Test Set Performance:")
print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1-macro: {test_f1_macro:.4f}")
print(f"Precision-macro: {test_precision_macro:.4f}")
print(f"Recall-macro: {test_recall_macro:.4f}")

# Add MobileViTv2 results to the models_data DataFrame
# Check if models_data DataFrame exists, if not, create it.
if 'models_data' not in locals() or models_data is None:
    models_data = pd.DataFrame()
elif not isinstance(models_data, pd.DataFrame):
     models_data = pd.DataFrame(models_data)


mobilevit_model_data = {
    'model_name': 'MobileViTv2 Original Data (Train + Val)',
    'epochs_run': len(mobilevit_history['train_loss']),
    'early_stopping_epoch': len(mobilevit_history['train_loss']) - epochs_no_improve, # Approximate early stopping epoch
    'fit_time': sum([(mobilevit_history['train_loss'][i] + mobilevit_history['val_loss'][i]) for i in range(len(mobilevit_history['train_loss']))]), # Placeholder, needs actual time per epoch if tracked
    'Train Accuracy (Hist)': mobilevit_history['train_accuracy'][-1],
    'Val Accuracy (Hist)': mobilevit_history['val_accuracy'][-1],
    'Train f1_macro (Hist)': mobilevit_history['train_f1_macro'][-1],
    'Val f1_macro (Hist)': mobilevit_history['val_f1_macro'][-1],
    'Val Accuracy (Sklearn)': test_accuracy, # Using test here as it's the final evaluation
    'Val Precision (Sklearn)': test_precision_macro,
    'Val Recall (Sklearn)': test_recall_macro,
    'Val f1_macro (Sklearn)': test_f1_macro,
    'Val f2_macro (Sklearn)': fbeta_score(test_labels, test_preds, beta=2, average='macro', zero_division=0) # Calculate f2_macro for test set
}


# Convert to DataFrame and concatenate
mobilevit_df = pd.DataFrame([mobilevit_model_data])
models_data = pd.concat([models_data, mobilevit_df], ignore_index=True)

print("\nUpdated Model Comparison Data:")
print(models_data)

# Optionally save the updated models_data DataFrame
timestamp_compare_all = datetime.now().strftime("%Y%m%d_%H%M%S")
filename_compare_all = f"model_compare_all_{timestamp_compare_all}.csv"
models_data.to_csv(save_path + '/' + filename_compare_all, index=False)
print(f"\nModel comparison data saved to {save_path}/{filename_compare_all}")

test_labels, test_preds = evaluate(model, test_loader)
print("Test classification report:")
print(classification_report(test_labels, test_preds))

plt.figure(figsize=(6,4))
cm = confusion_matrix(test_labels, test_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix MobileViTv2 - Test')
plt.savefig(save_path + '/' + 'Confusion matrix MobileViTv2 - Test2.png', bbox_inches='tight')
plt.show()



test_labels, test_preds = evaluate(model, test_loader)
print("Test classification report:")
print(classification_report(test_labels, test_preds))

plt.figure(figsize=(6,4))
cm = confusion_matrix(test_labels, test_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix MobileViTv2 - Test')
plt.savefig(save_path + '/' + 'Confusion matrix MobileViTv2 - Test.png', bbox_inches='tight')
plt.show()





"""## 4.7 - Feature importance"""

rs=13

"""### 4.7.1 - Global feature importance"""

import os
from PIL import Image
import numpy as np
import torch

# Path to your images
background_images_dir = '/content/drive/My Drive/Colab Notebooks/OCDS_P6&P8/flipkart_images'

# List up to 70 image files in the directory
image_files = [os.path.join(background_images_dir, f) for f in os.listdir(background_images_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
background_image_files = image_files[:50]  # First 50 for background
test_image_files = image_files[50:70] if len(image_files) > 50 else image_files[:20]  # Next 20 for test

def preprocess_image(image_path):
    image = Image.open(image_path).convert('RGB')
    image = image.resize((224, 224))
    image_np = np.array(image).astype(np.float32) / 255.0
    image_np = np.transpose(image_np, (2, 0, 1))  # (C, H, W)
    return image_np

background_images = torch.tensor(np.stack([preprocess_image(p) for p in background_image_files]), dtype=torch.float32)  # (N, 3, 224, 224)
test_images_subset = torch.tensor(np.stack([preprocess_image(p) for p in test_image_files]), dtype=torch.float32)  # (M, 3, 224, 224)

from functools import partial

def replace_silu_inplace(module):
    for name, child in module.named_children():
        if isinstance(child, nn.SiLU) and child.inplace:
            setattr(module, name, nn.SiLU(inplace=False))
        replace_silu_inplace(child)

# Apply to your model
replace_silu_inplace(model)

"""# optimized code"""

import torch
import numpy as np
import shap

# Reduce background and test samples
background_images = background_images[:10]  # Use only 10 images
test_images_subset = test_images_subset[:5]  # Use only 5 images

# Optionally, resize images for SHAP analysis (not for training)
background_images_small = torch.nn.functional.interpolate(
    background_images, size=(112, 112), mode='bilinear'
)
test_images_subset_small = torch.nn.functional.interpolate(
    test_images_subset, size=(112, 112), mode='bilinear'
)

# Flatten for KernelExplainer
background_flat = background_images_small.cpu().numpy().reshape(len(background_images_small), -1)
test_flat = test_images_subset_small.cpu().numpy().reshape(len(test_images_subset_small), -1)

def predict_fn(images_flat):
    images = torch.tensor(
        images_flat.reshape(-1, 3, 112, 112), dtype=torch.float32
    ).to(device)
    with torch.no_grad():
        outputs = model(images)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
    return probabilities.cpu().numpy()

explainer = shap.KernelExplainer(predict_fn, background_flat)

shap_values = explainer.shap_values(test_flat, nsamples=100)  # Lower nsamples for speed

shap.summary_plot(
    shap_values,
    test_flat,
    plot_type="bar",
    feature_names=[f"Pixel_{i}" for i in range(test_flat.shape[1])],
    class_names=list_labels
)

import shap
import torch

# Ensure model and background images are on the same device
model.to(device)
model.eval()
background_images = background_images.to(device)
test_images = test_images.to(device)

# Create DeepExplainer
explainer = shap.DeepExplainer(model, background_images)

# Compute SHAP values for test images
shap_values = explainer.shap_values(test_images)

shap.summary_plot(
    shap_values,
    test_flat,
    plot_type="bar",
    feature_names=[f"Pixel_{i}" for i in range(test_flat.shape[1])],
    class_names=list_labels
)













import shap

# Make sure your model is on the right device and in eval mode
model.to(device)
model.eval()

# Move images to device
background_images = background_images.to(device)
test_images_subset = test_images_subset.to(device)

# Create SHAP DeepExplainer and compute SHAP values
explainer = shap.DeepExplainer(model, background_images)
shap_values = explainer.shap_values(test_images_subset)

# Plot global feature importance
shap.summary_plot(
    shap_values,
    np.transpose(test_images_subset.cpu().numpy(), (0, 2, 3, 1)),  # (N, H, W, C)
    plot_type="bar",
    class_names=list_labels  # e.g., ['Class_0', ..., 'Class_6']
)

# 1. SHAP Summary Plot (Global Feature Importance)



# Use DeepExplainer (optimized for CNNs)
explainer = shap.DeepExplainer(model, background_images.to(device))  # Background: 50-100 images
shap_values = explainer.shap_values(test_images_subset.to(device))   # Test subset: ~100 images

# Summary plot for global importance (aggregates absolute SHAP values)
shap.summary_plot(
    shap_values,
    np.transpose(test_images_subset.cpu().numpy(), (0, 2, 3, 1)),  # Shape: (N, H, W, C)
    plot_type="bar",  # Shows mean |SHAP| per feature
    class_names=list_labels  # Your 7 class names
)

# 2. Mean Absolute SHAP Values

# For multiclass: Average across all classes and samples
mean_shap = np.mean([np.abs(shap_values[i]) for i in range(7)], axis=(0, 1))

# Reshape to image dimensions (H, W, C)
mean_shap_image = mean_shap.reshape(224, 224, 3)

# Plot heatmap
shap.image_plot([mean_shap_image], [np.transpose(test_images_subset[0].cpu().numpy(), (1, 2, 0))])

# 3. Class-Specific Global Importance
for class_idx in range(7):
    class_shap = np.mean(np.abs(shap_values[class_idx]), axis=0)
    class_shap_image = class_shap.reshape(224, 224, 3)

    shap.image_plot(
        [class_shap_image],
        [np.transpose(test_images_subset[0].cpu().numpy(), (1, 2, 0))],
        titles=[f"Class {list_labels[class_idx]} Importance"]
    )

# 4. Superpixel-Based Aggregation
from skimage.segmentation import slic

# Compute superpixels for an example image
image = test_images_subset[0].cpu().numpy().transpose(1, 2, 0)
segments = slic(image, n_segments=50, compactness=10)

# Aggregate SHAP values per superpixel
superpixel_shap = np.zeros(segments.max() + 1)
for i in np.unique(segments):
    superpixel_shap[i] = np.mean(mean_shap_image[segments == i])

# Visualize
shap.image_plot(
    [segments],
    [image],
    np.expand_dims(superpixel_shap, axis=0)
)







"""The error  
```
AttributeError: 'KernelExplainer' object has no attribute 'shap_interaction_values'
```
occurs because **SHAP KernelExplainer does not support `.shap_interaction_values`**. This feature is only available for certain explainers, primarily `TreeExplainer` (for tree-based models like XGBoost, LightGBM, etc.)[2][3][6][7]. Currently, neither `KernelExplainer` nor `DeepExplainer` supports direct computation of SHAP interaction values for deep learning models such as MobileViTv2[2][5].

---

## **What does this mean for your MobileViTv2 workflow?**

- **You cannot use `shap_interaction_values` with KernelExplainer or DeepExplainer.**
- The type of plot you showed (SHAP interaction value summary plot) is only natively supported for tree models using `TreeExplainer`.
- For deep learning models (like MobileViTv2), only standard SHAP values (main effects) are supported out-of-the-box with `KernelExplainer` and `DeepExplainer`.

---

## **What can you do instead?**

### 1. **Standard SHAP Value Analysis**
You can still compute and visualize standard SHAP values (feature attributions) for your images using `explainer.shap_values` and plots like `shap.summary_plot` or `shap.image_plot`.

### 2. **Manual Approximation (Advanced)**
If you truly need interaction values for a deep model, you would have to implement a custom approximation, which is non-trivial and not supported by the SHAP API[4][5]. There is no official workaround in the SHAP library as of now.

### 3. **Alternative Libraries**
Some research libraries (e.g., [shapiq](https://christophm.github.io/interpretable-ml-book/shap.html)) are exploring more general interaction value estimation, but these are not yet mainstream or as plug-and-play as SHAP for tree models[4].

---

## **Summary Table**

| SHAP Explainer         | SHAP Values Supported | SHAP Interaction Values Supported |
|------------------------|----------------------|-----------------------------------|
| TreeExplainer          | Yes                  | Yes                               |
| KernelExplainer        | Yes                  | **No**                            |
| DeepExplainer          | Yes                  | **No**                            |

---

## **References**
- [2] GitHub Issue: KernelExplainer does not support shap_interaction_values
- [3] SHAP documentation: Interaction values for tree models
- [4] Interpretable ML Book: SHAP interactions
- [5] GitHub Issue: DeepExplainer does not support shap_interaction_values
- [6][7] SHAP documentation, TreeExplainer

---

**In summary:**  
You cannot generate SHAP interaction value plots for MobileViTv2 using KernelExplainer or DeepExplainer. You can only use standard SHAP value plots for deep learning models. Interaction value plots are only supported for tree-based models with TreeExplainer.

[1] https://pplx-res.cloudinary.com/image/private/user_uploads/31737542/22867266-f893-48c7-89e3-511fd79a66a2/shap_interaction_values.jpg
[2] https://github.com/slundberg/shap/issues/2579
[3] https://github.com/shap/shap
[4] https://christophm.github.io/interpretable-ml-book/shap.html
[5] https://github.com/slundberg/shap/issues/272
[6] https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Basic%20SHAP%20Interaction%20Value%20Example%20in%20XGBoost.html
[7] https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html
[8] https://github.com/slundberg/shap/issues/136
[9] https://pypi.org/project/shap/
[10] https://www.kaggle.com/code/bextuychiev/model-explainability-with-shap-only-guide-u-need
[11] https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html
[12] https://stackoverflow.com/questions/71861157/shap-kernelexplainer-attributeerror-numpy-ndarray
[13] https://shap.readthedocs.io/en/latest/generated/shap.GPUTreeExplainer.html
[14] https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb/
[15] https://docs.seldon.io/projects/alibi/en/latest/api/alibi.explainers.shap_wrappers.html
[16] https://shap.readthedocs.io/en/latest/generated/shap.KernelExplainer.html
[17] https://www.datacamp.com/tutorial/introduction-to-shap-values-machine-learning-interpretability
[18] https://stackoverflow.com/questions/75275838/r-how-to-calculate-and-plot-shap-interaction-values
[19] https://huggingface.co/docs/transformers/main/model_doc/mobilevitv2
"""







"""### 4.7.2 - Local feature importance

use image f4d4c2eec77732f56e47722d7a355f2b.jpg ligne 97 from Class 3 - Home Decor & Festive Needs
"""

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
image_path = '/content/drive/My Drive/Colab Notebooks/OCDS_P6&P8/flipkart_images/f4d4c2eec77732f56e47722d7a355f2b.jpg'

# --- Load image in OpenCV-compatible format (H, W, C) ---
def preprocess_image_for_shap(image_path):
    image = Image.open(image_path).convert("RGB")
    image = image.resize((224, 224))
    image_np = np.array(image)  # Shape: (224, 224, 3), dtype=uint8
    return np.expand_dims(image_np, axis=0)  # Add batch dim: (1, 224, 224, 3)

img = preprocess_image_for_shap(image_path)  # Shape: (1, 224, 224, 3)

# --- Define prediction function ---
def predict_fn(images_numpy):
    # Input shape: (N, H, W, 3) in [0,255]
    # Convert to (N, C, H, W) and normalize for model
    images_normalized = images_numpy.transpose(0, 3, 1, 2).astype(np.float32) / 255.0
    images_tensor = torch.tensor(images_normalized).to(device)

    with torch.no_grad():
        outputs = model(images_tensor)
        logits = outputs.logits if hasattr(outputs, "logits") else outputs
        probabilities = torch.nn.functional.softmax(logits, dim=1)
    return probabilities.cpu().numpy()

# --- Configure SHAP masker ---
masker = shap.maskers.Image("inpaint_telea", img.shape[1:])  # (224, 224, 3)

# --- Create SHAP explainer ---
list_labels = [f'Class_{i}' for i in range(7)]
explainer = shap.Explainer(predict_fn, masker, output_names=list_labels)

# --- Compute SHAP values ---
print("Computing SHAP values...")
shap_values = explainer(
    img,
    max_evals=500,  # Reduce if memory-constrained
    batch_size=25,
    outputs=shap.Explanation.argsort.flip[:1]
)

# --- Visualize ---
plt.figure(figsize=(10,10))
shap.image_plot(shap_values, img)
plt.savefig(save_path + '/' + 'SHAP values buddha - MobileViTv2.png', bbox_inches='tight')
plt.show()