digraph {
	graph [size="195.6,195.6"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	138217673228944 [label="
 (1, 7)" fillcolor=darkolivegreen1]
	138217804090864 [label=AddmmBackward0]
	138217804088512 -> 138217804090864
	138228947466288 [label="head.fc.bias
 (7)" fillcolor=lightblue]
	138228947466288 -> 138217804088512
	138217804088512 [label=AccumulateGrad]
	138217804088416 -> 138217804090864
	138217804088416 [label=ViewBackward0]
	138217804090384 -> 138217804088416
	138217804090384 [label=MeanBackward1]
	138217804090288 -> 138217804090384
	138217804090288 [label=NativeBatchNormBackward0]
	138217804088320 -> 138217804090288
	138217804088320 [label=ConvolutionBackward0]
	138217804091584 -> 138217804088320
	138217804091584 [label=UnsafeViewBackward0]
	138217804091632 -> 138217804091584
	138217804091632 [label=CloneBackward0]
	138217804087456 -> 138217804091632
	138217804087456 [label=PermuteBackward0]
	138217804088752 -> 138217804087456
	138217804088752 [label=ViewBackward0]
	138217804089136 -> 138217804088752
	138217804089136 [label=NativeGroupNormBackward0]
	138217804090768 -> 138217804089136
	138217804090768 [label=AddBackward0]
	138219867354496 -> 138217804090768
	138219867354496 [label=AddBackward0]
	138219867355408 -> 138219867354496
	138219867355408 [label=AddBackward0]
	138219867354832 -> 138219867355408
	138219867354832 [label=AddBackward0]
	138219867352000 -> 138219867354832
	138219867352000 [label=AddBackward0]
	138219867353584 -> 138219867352000
	138219867353584 [label=AddBackward0]
	138219867355552 -> 138219867353584
	138219867355552 [label=UnsafeViewBackward0]
	138219867351040 -> 138219867355552
	138219867351040 [label=CloneBackward0]
	138219867350944 -> 138219867351040
	138219867350944 [label=PermuteBackward0]
	138219867350800 -> 138219867350944
	138219867350800 [label=ViewBackward0]
	138219867350464 -> 138219867350800
	138219867350464 [label=ConvolutionBackward0]
	138219867350368 -> 138219867350464
	138219867350368 [label=SiluBackward0]
	138219867349072 -> 138219867350368
	138219867349072 [label=NativeBatchNormBackward0]
	138219867348064 -> 138219867349072
	138219867348064 [label=ConvolutionBackward0]
	138219867347440 -> 138219867348064
	138219867347440 [label=NativeBatchNormBackward0]
	138219867347296 -> 138219867347440
	138219867347296 [label=ConvolutionBackward0]
	138219867347008 -> 138219867347296
	138219867347008 [label=SiluBackward0]
	138219867346192 -> 138219867347008
	138219867346192 [label=NativeBatchNormBackward0]
	138219867346096 -> 138219867346192
	138219867346096 [label=ConvolutionBackward0]
	138219867343120 -> 138219867346096
	138219867343120 [label=SiluBackward0]
	138219867341872 -> 138219867343120
	138219867341872 [label=NativeBatchNormBackward0]
	138219867341776 -> 138219867341872
	138219867341776 [label=ConvolutionBackward0]
	138217804218080 -> 138219867341776
	138217804218080 [label=NativeBatchNormBackward0]
	138217804217744 -> 138217804218080
	138217804217744 [label=ConvolutionBackward0]
	138217804217360 -> 138217804217744
	138217804217360 [label=UnsafeViewBackward0]
	138217804217216 -> 138217804217360
	138217804217216 [label=CloneBackward0]
	138217804217120 -> 138217804217216
	138217804217120 [label=PermuteBackward0]
	138217804217024 -> 138217804217120
	138217804217024 [label=ViewBackward0]
	138217804216880 -> 138217804217024
	138217804216880 [label=NativeGroupNormBackward0]
	138217804216736 -> 138217804216880
	138217804216736 [label=AddBackward0]
	138217804216352 -> 138217804216736
	138217804216352 [label=AddBackward0]
	138217804216208 -> 138217804216352
	138217804216208 [label=AddBackward0]
	138217804216064 -> 138217804216208
	138217804216064 [label=AddBackward0]
	138217804215920 -> 138217804216064
	138217804215920 [label=AddBackward0]
	138217804215632 -> 138217804215920
	138217804215632 [label=AddBackward0]
	138217804215344 -> 138217804215632
	138217804215344 [label=AddBackward0]
	138217804215104 -> 138217804215344
	138217804215104 [label=AddBackward0]
	138217804214384 -> 138217804215104
	138217804214384 [label=UnsafeViewBackward0]
	138217804213520 -> 138217804214384
	138217804213520 [label=CloneBackward0]
	138217804213328 -> 138217804213520
	138217804213328 [label=PermuteBackward0]
	138217804213136 -> 138217804213328
	138217804213136 [label=ViewBackward0]
	138217804213040 -> 138217804213136
	138217804213040 [label=ConvolutionBackward0]
	138217804212896 -> 138217804213040
	138217804212896 [label=SiluBackward0]
	138217804212752 -> 138217804212896
	138217804212752 [label=NativeBatchNormBackward0]
	138217804212512 -> 138217804212752
	138217804212512 [label=ConvolutionBackward0]
	138217804212224 -> 138217804212512
	138217804212224 [label=NativeBatchNormBackward0]
	138217804212080 -> 138217804212224
	138217804212080 [label=ConvolutionBackward0]
	138217804211888 -> 138217804212080
	138217804211888 [label=SiluBackward0]
	138217804211744 -> 138217804211888
	138217804211744 [label=NativeBatchNormBackward0]
	138217804211552 -> 138217804211744
	138217804211552 [label=ConvolutionBackward0]
	138217804211216 -> 138217804211552
	138217804211216 [label=SiluBackward0]
	138217804211072 -> 138217804211216
	138217804211072 [label=NativeBatchNormBackward0]
	138217804210976 -> 138217804211072
	138217804210976 [label=ConvolutionBackward0]
	138217804210688 -> 138217804210976
	138217804210688 [label=NativeBatchNormBackward0]
	138217804210544 -> 138217804210688
	138217804210544 [label=ConvolutionBackward0]
	138217804210112 -> 138217804210544
	138217804210112 [label=UnsafeViewBackward0]
	138217804209632 -> 138217804210112
	138217804209632 [label=CloneBackward0]
	138217804209440 -> 138217804209632
	138217804209440 [label=PermuteBackward0]
	138217804202096 -> 138217804209440
	138217804202096 [label=ViewBackward0]
	138217804202048 -> 138217804202096
	138217804202048 [label=NativeGroupNormBackward0]
	138217677664272 -> 138217804202048
	138217677664272 [label=AddBackward0]
	138217677662496 -> 138217677664272
	138217677662496 [label=AddBackward0]
	138217677661824 -> 138217677662496
	138217677661824 [label=AddBackward0]
	138217677665616 -> 138217677661824
	138217677665616 [label=AddBackward0]
	138217677664608 -> 138217677665616
	138217677664608 [label=UnsafeViewBackward0]
	138217677661584 -> 138217677664608
	138217677661584 [label=CloneBackward0]
	138217677667056 -> 138217677661584
	138217677667056 [label=PermuteBackward0]
	138217809603136 -> 138217677667056
	138217809603136 [label=ViewBackward0]
	138217809594160 -> 138217809603136
	138217809594160 [label=ConvolutionBackward0]
	138217809596128 -> 138217809594160
	138217809596128 [label=SiluBackward0]
	138217809608464 -> 138217809596128
	138217809608464 [label=NativeBatchNormBackward0]
	138217809608176 -> 138217809608464
	138217809608176 [label=ConvolutionBackward0]
	138217809606784 -> 138217809608176
	138217809606784 [label=NativeBatchNormBackward0]
	138217809604576 -> 138217809606784
	138217809604576 [label=ConvolutionBackward0]
	138217809604288 -> 138217809604576
	138217809604288 [label=SiluBackward0]
	138217809604096 -> 138217809604288
	138217809604096 [label=NativeBatchNormBackward0]
	138217809603952 -> 138217809604096
	138217809603952 [label=ConvolutionBackward0]
	138217809603280 -> 138217809603952
	138217809603280 [label=SiluBackward0]
	138217809603040 -> 138217809603280
	138217809603040 [label=NativeBatchNormBackward0]
	138217809602560 -> 138217809603040
	138217809602560 [label=ConvolutionBackward0]
	138217809600784 -> 138217809602560
	138217809600784 [label=AddBackward0]
	138217809600208 -> 138217809600784
	138217809600208 [label=NativeBatchNormBackward0]
	138217809596224 -> 138217809600208
	138217809596224 [label=ConvolutionBackward0]
	138217809594928 -> 138217809596224
	138217809594928 [label=SiluBackward0]
	138217809594688 -> 138217809594928
	138217809594688 [label=NativeBatchNormBackward0]
	138217809594592 -> 138217809594688
	138217809594592 [label=ConvolutionBackward0]
	138217809594400 -> 138217809594592
	138217809594400 [label=SiluBackward0]
	138217809593680 -> 138217809594400
	138217809593680 [label=NativeBatchNormBackward0]
	138217809593440 -> 138217809593680
	138217809593440 [label=ConvolutionBackward0]
	138217809600256 -> 138217809593440
	138217809600256 [label=NativeBatchNormBackward0]
	138217809592816 -> 138217809600256
	138217809592816 [label=ConvolutionBackward0]
	138217809592384 -> 138217809592816
	138217809592384 [label=SiluBackward0]
	138217809607120 -> 138217809592384
	138217809607120 [label=NativeBatchNormBackward0]
	138217809601840 -> 138217809607120
	138217809601840 [label=ConvolutionBackward0]
	138217809596080 -> 138217809601840
	138217809596080 [label=SiluBackward0]
	138217809595984 -> 138217809596080
	138217809595984 [label=NativeBatchNormBackward0]
	138217809594352 -> 138217809595984
	138217809594352 [label=ConvolutionBackward0]
	138217809600640 -> 138217809594352
	138217809600640 [label=NativeBatchNormBackward0]
	138217809602848 -> 138217809600640
	138217809602848 [label=ConvolutionBackward0]
	138217809607888 -> 138217809602848
	138217809607888 [label=SiluBackward0]
	138217809604480 -> 138217809607888
	138217809604480 [label=NativeBatchNormBackward0]
	138217809593056 -> 138217809604480
	138217809593056 [label=ConvolutionBackward0]
	138217809593488 -> 138217809593056
	138217809593488 [label=SiluBackward0]
	138217809603184 -> 138217809593488
	138217809603184 [label=NativeBatchNormBackward0]
	138217679154256 -> 138217809603184
	138217679154256 [label=ConvolutionBackward0]
	138217679153920 -> 138217679154256
	138217679153920 [label=SiluBackward0]
	138217679153440 -> 138217679153920
	138217679153440 [label=NativeBatchNormBackward0]
	138217679153296 -> 138217679153440
	138217679153296 [label=ConvolutionBackward0]
	138217679143792 -> 138217679153296
	138221981677232 [label="stem.conv.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	138221981677232 -> 138217679143792
	138217679143792 [label=AccumulateGrad]
	138217679153392 -> 138217679153440
	138221981676560 [label="stem.bn.weight
 (16)" fillcolor=lightblue]
	138221981676560 -> 138217679153392
	138217679153392 [label=AccumulateGrad]
	138217679153680 -> 138217679153440
	138221981672048 [label="stem.bn.bias
 (16)" fillcolor=lightblue]
	138221981672048 -> 138217679153680
	138217679153680 [label=AccumulateGrad]
	138217679153968 -> 138217679154256
	138221981665808 [label="stages.0.0.conv1_1x1.conv.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	138221981665808 -> 138217679153968
	138217679153968 [label=AccumulateGrad]
	138217679154304 -> 138217809603184
	138221982096976 [label="stages.0.0.conv1_1x1.bn.weight
 (32)" fillcolor=lightblue]
	138221982096976 -> 138217679154304
	138217679154304 [label=AccumulateGrad]
	138217679157280 -> 138217809603184
	138221982096784 [label="stages.0.0.conv1_1x1.bn.bias
 (32)" fillcolor=lightblue]
	138221982096784 -> 138217679157280
	138217679157280 [label=AccumulateGrad]
	138217809593248 -> 138217809593056
	138221982094960 [label="stages.0.0.conv2_kxk.conv.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	138221982094960 -> 138217809593248
	138217809593248 [label=AccumulateGrad]
	138217809595888 -> 138217809604480
	138221982096400 [label="stages.0.0.conv2_kxk.bn.weight
 (32)" fillcolor=lightblue]
	138221982096400 -> 138217809595888
	138217809595888 [label=AccumulateGrad]
	138217809603904 -> 138217809604480
	138221982095440 [label="stages.0.0.conv2_kxk.bn.bias
 (32)" fillcolor=lightblue]
	138221982095440 -> 138217809603904
	138217809603904 [label=AccumulateGrad]
	138217809606928 -> 138217809602848
	138221982093808 [label="stages.0.0.conv3_1x1.conv.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	138221982093808 -> 138217809606928
	138217809606928 [label=AccumulateGrad]
	138217809603616 -> 138217809600640
	138221982096688 [label="stages.0.0.conv3_1x1.bn.weight
 (32)" fillcolor=lightblue]
	138221982096688 -> 138217809603616
	138217809603616 [label=AccumulateGrad]
	138217809595744 -> 138217809600640
	138221982096016 [label="stages.0.0.conv3_1x1.bn.bias
 (32)" fillcolor=lightblue]
	138221982096016 -> 138217809595744
	138217809595744 [label=AccumulateGrad]
	138217809607360 -> 138217809594352
	138221982096496 [label="stages.1.0.conv1_1x1.conv.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	138221982096496 -> 138217809607360
	138217809607360 [label=AccumulateGrad]
	138217809596464 -> 138217809595984
	138221982096304 [label="stages.1.0.conv1_1x1.bn.weight
 (64)" fillcolor=lightblue]
	138221982096304 -> 138217809596464
	138217809596464 [label=AccumulateGrad]
	138217809608320 -> 138217809595984
	138221982089776 [label="stages.1.0.conv1_1x1.bn.bias
 (64)" fillcolor=lightblue]
	138221982089776 -> 138217809608320
	138217809608320 [label=AccumulateGrad]
	138217809608656 -> 138217809601840
	138221982095152 [label="stages.1.0.conv2_kxk.conv.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	138221982095152 -> 138217809608656
	138217809608656 [label=AccumulateGrad]
	138217809600736 -> 138217809607120
	138221982095056 [label="stages.1.0.conv2_kxk.bn.weight
 (64)" fillcolor=lightblue]
	138221982095056 -> 138217809600736
	138217809600736 [label=AccumulateGrad]
	138217809608608 -> 138217809607120
	138221982094576 [label="stages.1.0.conv2_kxk.bn.bias
 (64)" fillcolor=lightblue]
	138221982094576 -> 138217809608608
	138217809608608 [label=AccumulateGrad]
	138217809592432 -> 138217809592816
	138221982093520 [label="stages.1.0.conv3_1x1.conv.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	138221982093520 -> 138217809592432
	138217809592432 [label=AccumulateGrad]
	138217809592864 -> 138217809600256
	138221982093424 [label="stages.1.0.conv3_1x1.bn.weight
 (64)" fillcolor=lightblue]
	138221982093424 -> 138217809592864
	138217809592864 [label=AccumulateGrad]
	138217809593152 -> 138217809600256
	138221982092944 [label="stages.1.0.conv3_1x1.bn.bias
 (64)" fillcolor=lightblue]
	138221982092944 -> 138217809593152
	138217809593152 [label=AccumulateGrad]
	138217809592912 -> 138217809593440
	138221982091792 [label="stages.1.1.conv1_1x1.conv.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	138221982091792 -> 138217809592912
	138217809592912 [label=AccumulateGrad]
	138217809593536 -> 138217809593680
	138221982091696 [label="stages.1.1.conv1_1x1.bn.weight
 (128)" fillcolor=lightblue]
	138221982091696 -> 138217809593536
	138217809593536 [label=AccumulateGrad]
	138217809594304 -> 138217809593680
	138221982091216 [label="stages.1.1.conv1_1x1.bn.bias
 (128)" fillcolor=lightblue]
	138221982091216 -> 138217809594304
	138217809594304 [label=AccumulateGrad]
	138217809594448 -> 138217809594592
	138221982089584 [label="stages.1.1.conv2_kxk.conv.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	138221982089584 -> 138217809594448
	138217809594448 [label=AccumulateGrad]
	138217809594640 -> 138217809594688
	138221982089488 [label="stages.1.1.conv2_kxk.bn.weight
 (128)" fillcolor=lightblue]
	138221982089488 -> 138217809594640
	138217809594640 [label=AccumulateGrad]
	138217809594880 -> 138217809594688
	138228947533648 [label="stages.1.1.conv2_kxk.bn.bias
 (128)" fillcolor=lightblue]
	138228947533648 -> 138217809594880
	138217809594880 [label=AccumulateGrad]
	138217809594976 -> 138217809596224
	138228947533552 [label="stages.1.1.conv3_1x1.conv.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	138228947533552 -> 138217809594976
	138217809594976 [label=AccumulateGrad]
	138217809596320 -> 138217809600208
	138228947533264 [label="stages.1.1.conv3_1x1.bn.weight
 (64)" fillcolor=lightblue]
	138228947533264 -> 138217809596320
	138217809596320 [label=AccumulateGrad]
	138217809596368 -> 138217809600208
	138228947533360 [label="stages.1.1.conv3_1x1.bn.bias
 (64)" fillcolor=lightblue]
	138228947533360 -> 138217809596368
	138217809596368 [label=AccumulateGrad]
	138217809600256 -> 138217809600784
	138217809601120 -> 138217809602560
	138228947532592 [label="stages.2.0.conv1_1x1.conv.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	138228947532592 -> 138217809601120
	138217809601120 [label=AccumulateGrad]
	138217809602608 -> 138217809603040
	138228947532112 [label="stages.2.0.conv1_1x1.bn.weight
 (128)" fillcolor=lightblue]
	138228947532112 -> 138217809602608
	138217809602608 [label=AccumulateGrad]
	138217809603232 -> 138217809603040
	138228947532208 [label="stages.2.0.conv1_1x1.bn.bias
 (128)" fillcolor=lightblue]
	138228947532208 -> 138217809603232
	138217809603232 [label=AccumulateGrad]
	138217809603328 -> 138217809603952
	138228947531440 [label="stages.2.0.conv2_kxk.conv.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	138228947531440 -> 138217809603328
	138217809603328 [label=AccumulateGrad]
	138217809604048 -> 138217809604096
	138228947531536 [label="stages.2.0.conv2_kxk.bn.weight
 (128)" fillcolor=lightblue]
	138228947531536 -> 138217809604048
	138217809604048 [label=AccumulateGrad]
	138217809604192 -> 138217809604096
	138228947531056 [label="stages.2.0.conv2_kxk.bn.bias
 (128)" fillcolor=lightblue]
	138228947531056 -> 138217809604192
	138217809604192 [label=AccumulateGrad]
	138217809604336 -> 138217809604576
	138228947530960 [label="stages.2.0.conv3_1x1.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	138228947530960 -> 138217809604336
	138217809604336 [label=AccumulateGrad]
	138217809606688 -> 138217809606784
	138228947530480 [label="stages.2.0.conv3_1x1.bn.weight
 (128)" fillcolor=lightblue]
	138228947530480 -> 138217809606688
	138217809606688 [label=AccumulateGrad]
	138217809606736 -> 138217809606784
	138228947530576 [label="stages.2.0.conv3_1x1.bn.bias
 (128)" fillcolor=lightblue]
	138228947530576 -> 138217809606736
	138217809606736 [label=AccumulateGrad]
	138217809606832 -> 138217809608176
	138228947530000 [label="stages.2.1.conv_kxk.conv.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	138228947530000 -> 138217809606832
	138217809606832 [label=AccumulateGrad]
	138217809608416 -> 138217809608464
	138228947530096 [label="stages.2.1.conv_kxk.bn.weight
 (128)" fillcolor=lightblue]
	138228947530096 -> 138217809608416
	138217809608416 [label=AccumulateGrad]
	138217809603664 -> 138217809608464
	138228947529808 [label="stages.2.1.conv_kxk.bn.bias
 (128)" fillcolor=lightblue]
	138228947529808 -> 138217809603664
	138217809603664 [label=AccumulateGrad]
	138217809594256 -> 138217809594160
	138228947529520 [label="stages.2.1.conv_1x1.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	138228947529520 -> 138217809594256
	138217809594256 [label=AccumulateGrad]
	138217677665088 -> 138217677665616
	138217677665088 [label=ConvolutionBackward0]
	138217677663840 -> 138217677665088
	138217677663840 [label=MulBackward0]
	138217809608128 -> 138217677663840
	138217809608128 [label=ReluBackward0]
	138217809604432 -> 138217809608128
	138217809604432 [label=SplitWithSizesBackward0]
	138217809603472 -> 138217809604432
	138217809603472 [label=ConvolutionBackward0]
	138217809601552 -> 138217809603472
	138217809601552 [label=NativeGroupNormBackward0]
	138217677664608 -> 138217809601552
	138217809596032 -> 138217809601552
	138228947529424 [label="stages.2.1.transformer.0.norm1.weight
 (64)" fillcolor=lightblue]
	138228947529424 -> 138217809596032
	138217809596032 [label=AccumulateGrad]
	138217809595936 -> 138217809601552
	138228947529328 [label="stages.2.1.transformer.0.norm1.bias
 (64)" fillcolor=lightblue]
	138228947529328 -> 138217809595936
	138217809595936 [label=AccumulateGrad]
	138217809604144 -> 138217809603472
	138228947529136 [label="stages.2.1.transformer.0.attn.qkv_proj.weight
 (129, 64, 1, 1)" fillcolor=lightblue]
	138228947529136 -> 138217809604144
	138217809604144 [label=AccumulateGrad]
	138217809608512 -> 138217809603472
	138228947529232 [label="stages.2.1.transformer.0.attn.qkv_proj.bias
 (129)" fillcolor=lightblue]
	138228947529232 -> 138217809608512
	138217809608512 [label=AccumulateGrad]
	138217809607072 -> 138217677663840
	138217809607072 [label=ExpandBackward0]
	138217809602080 -> 138217809607072
	138217809602080 [label=SumBackward1]
	138217809600688 -> 138217809602080
	138217809600688 [label=MulBackward0]
	138217809604432 -> 138217809600688
	138217809594544 -> 138217809600688
	138217809594544 [label=SoftmaxBackward0]
	138217809604432 -> 138217809594544
	138217677665664 -> 138217677665088
	138228947528944 [label="stages.2.1.transformer.0.attn.out_proj.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	138228947528944 -> 138217677665664
	138217677665664 [label=AccumulateGrad]
	138217809595840 -> 138217677665088
	138228947529040 [label="stages.2.1.transformer.0.attn.out_proj.bias
 (64)" fillcolor=lightblue]
	138228947529040 -> 138217809595840
	138217809595840 [label=AccumulateGrad]
	138217677665712 -> 138217677661824
	138217677665712 [label=ConvolutionBackward0]
	138217677665376 -> 138217677665712
	138217677665376 [label=SiluBackward0]
	138217809593008 -> 138217677665376
	138217809593008 [label=ConvolutionBackward0]
	138217809603424 -> 138217809593008
	138217809603424 [label=NativeGroupNormBackward0]
	138217677665616 -> 138217809603424
	138217809592528 -> 138217809603424
	138228947528752 [label="stages.2.1.transformer.0.norm2.weight
 (64)" fillcolor=lightblue]
	138228947528752 -> 138217809592528
	138217809592528 [label=AccumulateGrad]
	138217809593584 -> 138217809603424
	138228947528848 [label="stages.2.1.transformer.0.norm2.bias
 (64)" fillcolor=lightblue]
	138228947528848 -> 138217809593584
	138217809593584 [label=AccumulateGrad]
	138217809594832 -> 138217809593008
	138228947528656 [label="stages.2.1.transformer.0.mlp.fc1.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	138228947528656 -> 138217809594832
	138217809594832 [label=AccumulateGrad]
	138217809603088 -> 138217809593008
	138228947528560 [label="stages.2.1.transformer.0.mlp.fc1.bias
 (128)" fillcolor=lightblue]
	138228947528560 -> 138217809603088
	138217809603088 [label=AccumulateGrad]
	138217809593824 -> 138217677665712
	138228947528368 [label="stages.2.1.transformer.0.mlp.fc2.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	138228947528368 -> 138217809593824
	138217809593824 [label=AccumulateGrad]
	138217809607936 -> 138217677665712
	138228947528464 [label="stages.2.1.transformer.0.mlp.fc2.bias
 (64)" fillcolor=lightblue]
	138228947528464 -> 138217809607936
	138217809607936 [label=AccumulateGrad]
	138217677664032 -> 138217677662496
	138217677664032 [label=ConvolutionBackward0]
	138217677668304 -> 138217677664032
	138217677668304 [label=MulBackward0]
	138217809600976 -> 138217677668304
	138217809600976 [label=ReluBackward0]
	138217809593776 -> 138217809600976
	138217809593776 [label=SplitWithSizesBackward0]
	138217809595792 -> 138217809593776
	138217809595792 [label=ConvolutionBackward0]
	138217809602896 -> 138217809595792
	138217809602896 [label=NativeGroupNormBackward0]
	138217677661824 -> 138217809602896
	138217809600304 -> 138217809602896
	138228947528176 [label="stages.2.1.transformer.1.norm1.weight
 (64)" fillcolor=lightblue]
	138228947528176 -> 138217809600304
	138217809600304 [label=AccumulateGrad]
	138217679154160 -> 138217809602896
	138228947528272 [label="stages.2.1.transformer.1.norm1.bias
 (64)" fillcolor=lightblue]
	138228947528272 -> 138217679154160
	138217679154160 [label=AccumulateGrad]
	138217809606976 -> 138217809595792
	138228947527984 [label="stages.2.1.transformer.1.attn.qkv_proj.weight
 (129, 64, 1, 1)" fillcolor=lightblue]
	138228947527984 -> 138217809606976
	138217809606976 [label=AccumulateGrad]
	138217809608560 -> 138217809595792
	138228947528080 [label="stages.2.1.transformer.1.attn.qkv_proj.bias
 (129)" fillcolor=lightblue]
	138228947528080 -> 138217809608560
	138217809608560 [label=AccumulateGrad]
	138217809592768 -> 138217677668304
	138217809592768 [label=ExpandBackward0]
	138217809603712 -> 138217809592768
	138217809603712 [label=SumBackward1]
	138217809604384 -> 138217809603712
	138217809604384 [label=MulBackward0]
	138217809593776 -> 138217809604384
	138217679145616 -> 138217809604384
	138217679145616 [label=SoftmaxBackward0]
	138217809593776 -> 138217679145616
	138217809594496 -> 138217677664032
	138228947527792 [label="stages.2.1.transformer.1.attn.out_proj.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	138228947527792 -> 138217809594496
	138217809594496 [label=AccumulateGrad]
	138217809604528 -> 138217677664032
	138228947527888 [label="stages.2.1.transformer.1.attn.out_proj.bias
 (64)" fillcolor=lightblue]
	138228947527888 -> 138217809604528
	138217809604528 [label=AccumulateGrad]
	138217677668064 -> 138217677664272
	138217677668064 [label=ConvolutionBackward0]
	138217677661872 -> 138217677668064
	138217677661872 [label=SiluBackward0]
	138217679143696 -> 138217677661872
	138217679143696 [label=ConvolutionBackward0]
	138217679154112 -> 138217679143696
	138217679154112 [label=NativeGroupNormBackward0]
	138217677662496 -> 138217679154112
	138217679143216 -> 138217679154112
	138228947527696 [label="stages.2.1.transformer.1.norm2.weight
 (64)" fillcolor=lightblue]
	138228947527696 -> 138217679143216
	138217679143216 [label=AccumulateGrad]
	138217679143648 -> 138217679154112
	138228947527600 [label="stages.2.1.transformer.1.norm2.bias
 (64)" fillcolor=lightblue]
	138228947527600 -> 138217679143648
	138217679143648 [label=AccumulateGrad]
	138217679153632 -> 138217679143696
	138228947527504 [label="stages.2.1.transformer.1.mlp.fc1.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	138228947527504 -> 138217679153632
	138217679153632 [label=AccumulateGrad]
	138217679155360 -> 138217679143696
	138228947527216 [label="stages.2.1.transformer.1.mlp.fc1.bias
 (128)" fillcolor=lightblue]
	138228947527216 -> 138217679155360
	138217679155360 [label=AccumulateGrad]
	138217809593728 -> 138217677668064
	138228947527408 [label="stages.2.1.transformer.1.mlp.fc2.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	138228947527408 -> 138217809593728
	138217809593728 [label=AccumulateGrad]
	138217809593296 -> 138217677668064
	138228947527312 [label="stages.2.1.transformer.1.mlp.fc2.bias
 (64)" fillcolor=lightblue]
	138228947527312 -> 138217809593296
	138217809593296 [label=AccumulateGrad]
	138217677665136 -> 138217804202048
	138228947527024 [label="stages.2.1.norm.weight
 (64)" fillcolor=lightblue]
	138228947527024 -> 138217677665136
	138217677665136 [label=AccumulateGrad]
	138217677661776 -> 138217804202048
	138228947527120 [label="stages.2.1.norm.bias
 (64)" fillcolor=lightblue]
	138228947527120 -> 138217677661776
	138217677661776 [label=AccumulateGrad]
	138217804210160 -> 138217804210544
	138228947525872 [label="stages.2.1.conv_proj.conv.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	138228947525872 -> 138217804210160
	138217804210160 [label=AccumulateGrad]
	138217804210592 -> 138217804210688
	138228947525776 [label="stages.2.1.conv_proj.bn.weight
 (128)" fillcolor=lightblue]
	138228947525776 -> 138217804210592
	138217804210592 [label=AccumulateGrad]
	138217804210640 -> 138217804210688
	138228947525968 [label="stages.2.1.conv_proj.bn.bias
 (128)" fillcolor=lightblue]
	138228947525968 -> 138217804210640
	138217804210640 [label=AccumulateGrad]
	138217804210736 -> 138217804210976
	138228947526544 [label="stages.3.0.conv1_1x1.conv.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	138228947526544 -> 138217804210736
	138217804210736 [label=AccumulateGrad]
	138217804211024 -> 138217804211072
	138228947526736 [label="stages.3.0.conv1_1x1.bn.weight
 (256)" fillcolor=lightblue]
	138228947526736 -> 138217804211024
	138217804211024 [label=AccumulateGrad]
	138217804211168 -> 138217804211072
	138228947526448 [label="stages.3.0.conv1_1x1.bn.bias
 (256)" fillcolor=lightblue]
	138228947526448 -> 138217804211168
	138217804211168 [label=AccumulateGrad]
	138217804211264 -> 138217804211552
	138228947525488 [label="stages.3.0.conv2_kxk.conv.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	138228947525488 -> 138217804211264
	138217804211264 [label=AccumulateGrad]
	138217804211696 -> 138217804211744
	138228947526832 [label="stages.3.0.conv2_kxk.bn.weight
 (256)" fillcolor=lightblue]
	138228947526832 -> 138217804211696
	138217804211696 [label=AccumulateGrad]
	138217804211840 -> 138217804211744
	138228947525584 [label="stages.3.0.conv2_kxk.bn.bias
 (256)" fillcolor=lightblue]
	138228947525584 -> 138217804211840
	138217804211840 [label=AccumulateGrad]
	138217804211936 -> 138217804212080
	138228947533072 [label="stages.3.0.conv3_1x1.conv.weight
 (192, 256, 1, 1)" fillcolor=lightblue]
	138228947533072 -> 138217804211936
	138217804211936 [label=AccumulateGrad]
	138217804212128 -> 138217804212224
	138228947532976 [label="stages.3.0.conv3_1x1.bn.weight
 (192)" fillcolor=lightblue]
	138228947532976 -> 138217804212128
	138217804212128 [label=AccumulateGrad]
	138217804212176 -> 138217804212224
	138228947532880 [label="stages.3.0.conv3_1x1.bn.bias
 (192)" fillcolor=lightblue]
	138228947532880 -> 138217804212176
	138217804212176 [label=AccumulateGrad]
	138217804212272 -> 138217804212512
	138228947531344 [label="stages.3.1.conv_kxk.conv.weight
 (192, 1, 3, 3)" fillcolor=lightblue]
	138228947531344 -> 138217804212272
	138217804212272 [label=AccumulateGrad]
	138217804212560 -> 138217804212752
	138228947531248 [label="stages.3.1.conv_kxk.bn.weight
 (192)" fillcolor=lightblue]
	138228947531248 -> 138217804212560
	138217804212560 [label=AccumulateGrad]
	138217804212848 -> 138217804212752
	138228947530768 [label="stages.3.1.conv_kxk.bn.bias
 (192)" fillcolor=lightblue]
	138228947530768 -> 138217804212848
	138217804212848 [label=AccumulateGrad]
	138217804212944 -> 138217804213040
	138221981895184 [label="stages.3.1.conv_1x1.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	138221981895184 -> 138217804212944
	138217804212944 [label=AccumulateGrad]
	138217804214624 -> 138217804215104
	138217804214624 [label=ConvolutionBackward0]
	138217809606592 -> 138217804214624
	138217809606592 [label=MulBackward0]
	138217804212464 -> 138217809606592
	138217804212464 [label=ReluBackward0]
	138217804211984 -> 138217804212464
	138217804211984 [label=SplitWithSizesBackward0]
	138217804211408 -> 138217804211984
	138217804211408 [label=ConvolutionBackward0]
	138217804211312 -> 138217804211408
	138217804211312 [label=NativeGroupNormBackward0]
	138217804214384 -> 138217804211312
	138217804210352 -> 138217804211312
	138221981897104 [label="stages.3.1.transformer.0.norm1.weight
 (96)" fillcolor=lightblue]
	138221981897104 -> 138217804210352
	138217804210352 [label=AccumulateGrad]
	138217804210400 -> 138217804211312
	138221981908912 [label="stages.3.1.transformer.0.norm1.bias
 (96)" fillcolor=lightblue]
	138221981908912 -> 138217804210400
	138217804210400 [label=AccumulateGrad]
	138217804211792 -> 138217804211408
	138221981897872 [label="stages.3.1.transformer.0.attn.qkv_proj.weight
 (193, 96, 1, 1)" fillcolor=lightblue]
	138221981897872 -> 138217804211792
	138217804211792 [label=AccumulateGrad]
	138217804212800 -> 138217804211408
	138221981900176 [label="stages.3.1.transformer.0.attn.qkv_proj.bias
 (193)" fillcolor=lightblue]
	138221981900176 -> 138217804212800
	138217804212800 [label=AccumulateGrad]
	138217804214048 -> 138217809606592
	138217804214048 [label=ExpandBackward0]
	138217804210880 -> 138217804214048
	138217804210880 [label=SumBackward1]
	138217804209536 -> 138217804210880
	138217804209536 [label=MulBackward0]
	138217804211984 -> 138217804209536
	138217804209872 -> 138217804209536
	138217804209872 [label=SoftmaxBackward0]
	138217804211984 -> 138217804209872
	138217804213472 -> 138217804214624
	138221981908720 [label="stages.3.1.transformer.0.attn.out_proj.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	138221981908720 -> 138217804213472
	138217804213472 [label=AccumulateGrad]
	138217804213904 -> 138217804214624
	138221981902096 [label="stages.3.1.transformer.0.attn.out_proj.bias
 (96)" fillcolor=lightblue]
	138221981902096 -> 138217804213904
	138217804213904 [label=AccumulateGrad]
	138217804215152 -> 138217804215344
	138217804215152 [label=ConvolutionBackward0]
	138217804213088 -> 138217804215152
	138217804213088 [label=SiluBackward0]
	138217804212416 -> 138217804213088
	138217804212416 [label=ConvolutionBackward0]
	138217679144224 -> 138217804212416
	138217679144224 [label=NativeGroupNormBackward0]
	138217804215104 -> 138217679144224
	138217679143072 -> 138217679144224
	138221981908816 [label="stages.3.1.transformer.0.norm2.weight
 (96)" fillcolor=lightblue]
	138221981908816 -> 138217679143072
	138217679143072 [label=AccumulateGrad]
	138217679143120 -> 138217679144224
	138221981904400 [label="stages.3.1.transformer.0.norm2.bias
 (96)" fillcolor=lightblue]
	138221981904400 -> 138217679143120
	138217679143120 [label=AccumulateGrad]
	138217679143744 -> 138217804212416
	138221981905936 [label="stages.3.1.transformer.0.mlp.fc1.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	138221981905936 -> 138217679143744
	138217679143744 [label=AccumulateGrad]
	138217679145568 -> 138217804212416
	138221981907856 [label="stages.3.1.transformer.0.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	138221981907856 -> 138217679145568
	138217679145568 [label=AccumulateGrad]
	138217804213280 -> 138217804215152
	138221981906320 [label="stages.3.1.transformer.0.mlp.fc2.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	138221981906320 -> 138217804213280
	138217804213280 [label=AccumulateGrad]
	138217804214672 -> 138217804215152
	138221981899408 [label="stages.3.1.transformer.0.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	138221981899408 -> 138217804214672
	138217804214672 [label=AccumulateGrad]
	138217804215440 -> 138217804215632
	138217804215440 [label=ConvolutionBackward0]
	138217804211120 -> 138217804215440
	138217804211120 [label=MulBackward0]
	138217679142976 -> 138217804211120
	138217679142976 [label=ReluBackward0]
	138217679155312 -> 138217679142976
	138217679155312 [label=SplitWithSizesBackward0]
	138217679159008 -> 138217679155312
	138217679159008 [label=ConvolutionBackward0]
	138217679158960 -> 138217679159008
	138217679158960 [label=NativeGroupNormBackward0]
	138217804215344 -> 138217679158960
	138217679154496 -> 138217679158960
	138221981907088 [label="stages.3.1.transformer.1.norm1.weight
 (96)" fillcolor=lightblue]
	138221981907088 -> 138217679154496
	138217679154496 [label=AccumulateGrad]
	138217679154448 -> 138217679158960
	138221981907568 [label="stages.3.1.transformer.1.norm1.bias
 (96)" fillcolor=lightblue]
	138221981907568 -> 138217679154448
	138217679154448 [label=AccumulateGrad]
	138217679159056 -> 138217679159008
	138221981905168 [label="stages.3.1.transformer.1.attn.qkv_proj.weight
 (193, 96, 1, 1)" fillcolor=lightblue]
	138221981905168 -> 138217679159056
	138217679159056 [label=AccumulateGrad]
	138217679144320 -> 138217679159008
	138221981903248 [label="stages.3.1.transformer.1.attn.qkv_proj.bias
 (193)" fillcolor=lightblue]
	138221981903248 -> 138217679144320
	138217679144320 [label=AccumulateGrad]
	138217679143024 -> 138217804211120
	138217679143024 [label=ExpandBackward0]
	138217679154352 -> 138217679143024
	138217679154352 [label=SumBackward1]
	138217679154736 -> 138217679154352
	138217679154736 [label=MulBackward0]
	138217679155312 -> 138217679154736
	138217679154592 -> 138217679154736
	138217679154592 [label=SoftmaxBackward0]
	138217679155312 -> 138217679154592
	138217804212032 -> 138217804215440
	138221981896720 [label="stages.3.1.transformer.1.attn.out_proj.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	138221981896720 -> 138217804212032
	138217804212032 [label=AccumulateGrad]
	138217804215248 -> 138217804215440
	138221981899024 [label="stages.3.1.transformer.1.attn.out_proj.bias
 (96)" fillcolor=lightblue]
	138221981899024 -> 138217804215248
	138217804215248 [label=AccumulateGrad]
	138217804215680 -> 138217804215920
	138217804215680 [label=ConvolutionBackward0]
	138217804215488 -> 138217804215680
	138217804215488 [label=SiluBackward0]
	138217679154640 -> 138217804215488
	138217679154640 [label=ConvolutionBackward0]
	138217679143312 -> 138217679154640
	138217679143312 [label=NativeGroupNormBackward0]
	138217804215632 -> 138217679143312
	138217679156704 -> 138217679143312
	138221981902864 [label="stages.3.1.transformer.1.norm2.weight
 (96)" fillcolor=lightblue]
	138221981902864 -> 138217679156704
	138217679156704 [label=AccumulateGrad]
	138217679156992 -> 138217679143312
	138221981895568 [label="stages.3.1.transformer.1.norm2.bias
 (96)" fillcolor=lightblue]
	138221981895568 -> 138217679156992
	138217679156992 [label=AccumulateGrad]
	138217679154688 -> 138217679154640
	138221981900560 [label="stages.3.1.transformer.1.mlp.fc1.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	138221981900560 -> 138217679154688
	138217679154688 [label=AccumulateGrad]
	138217679154400 -> 138217679154640
	138221981898256 [label="stages.3.1.transformer.1.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	138221981898256 -> 138217679154400
	138217679154400 [label=AccumulateGrad]
	138217679143168 -> 138217804215680
	138221981908048 [label="stages.3.1.transformer.1.mlp.fc2.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	138221981908048 -> 138217679143168
	138217679143168 [label=AccumulateGrad]
	138217679143264 -> 138217804215680
	138221981904016 [label="stages.3.1.transformer.1.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	138221981904016 -> 138217679143264
	138217679143264 [label=AccumulateGrad]
	138217804215968 -> 138217804216064
	138217804215968 [label=ConvolutionBackward0]
	138217804215776 -> 138217804215968
	138217804215776 [label=MulBackward0]
	138217679156128 -> 138217804215776
	138217679156128 [label=ReluBackward0]
	138217679156224 -> 138217679156128
	138217679156224 [label=SplitWithSizesBackward0]
	138217679155456 -> 138217679156224
	138217679155456 [label=ConvolutionBackward0]
	138217679155552 -> 138217679155456
	138217679155552 [label=NativeGroupNormBackward0]
	138217804215920 -> 138217679155552
	138217679155744 -> 138217679155552
	138221981901328 [label="stages.3.1.transformer.2.norm1.weight
 (96)" fillcolor=lightblue]
	138221981901328 -> 138217679155744
	138217679155744 [label=AccumulateGrad]
	138217679155696 -> 138217679155552
	138221981894416 [label="stages.3.1.transformer.2.norm1.bias
 (96)" fillcolor=lightblue]
	138221981894416 -> 138217679155696
	138217679155696 [label=AccumulateGrad]
	138217679155504 -> 138217679155456
	138221981894800 [label="stages.3.1.transformer.2.attn.qkv_proj.weight
 (193, 96, 1, 1)" fillcolor=lightblue]
	138221981894800 -> 138217679155504
	138217679155504 [label=AccumulateGrad]
	138217679156656 -> 138217679155456
	138221981894032 [label="stages.3.1.transformer.2.attn.qkv_proj.bias
 (193)" fillcolor=lightblue]
	138221981894032 -> 138217679156656
	138217679156656 [label=AccumulateGrad]
	138217679156176 -> 138217804215776
	138217679156176 [label=ExpandBackward0]
	138217677661344 -> 138217679156176
	138217677661344 [label=SumBackward1]
	138217804209824 -> 138217677661344
	138217804209824 [label=MulBackward0]
	138217679156224 -> 138217804209824
	138217679155648 -> 138217804209824
	138217679155648 [label=SoftmaxBackward0]
	138217679156224 -> 138217679155648
	138217679154544 -> 138217804215968
	138221981896336 [label="stages.3.1.transformer.2.attn.out_proj.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	138221981896336 -> 138217679154544
	138217679154544 [label=AccumulateGrad]
	138217679155264 -> 138217804215968
	138221981899792 [label="stages.3.1.transformer.2.attn.out_proj.bias
 (96)" fillcolor=lightblue]
	138221981899792 -> 138217679155264
	138217679155264 [label=AccumulateGrad]
	138217804216112 -> 138217804216208
	138217804216112 [label=ConvolutionBackward0]
	138217677665760 -> 138217804216112
	138217677665760 [label=SiluBackward0]
	138217679155984 -> 138217677665760
	138217679155984 [label=ConvolutionBackward0]
	138217679156752 -> 138217679155984
	138217679156752 [label=NativeGroupNormBackward0]
	138217804216064 -> 138217679156752
	138217679156416 -> 138217679156752
	138221981901712 [label="stages.3.1.transformer.2.norm2.weight
 (96)" fillcolor=lightblue]
	138221981901712 -> 138217679156416
	138217679156416 [label=AccumulateGrad]
	138217679156368 -> 138217679156752
	138221981907664 [label="stages.3.1.transformer.2.norm2.bias
 (96)" fillcolor=lightblue]
	138221981907664 -> 138217679156368
	138217679156368 [label=AccumulateGrad]
	138217679155936 -> 138217679155984
	138221981908528 [label="stages.3.1.transformer.2.mlp.fc1.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	138221981908528 -> 138217679155936
	138217679155936 [label=AccumulateGrad]
	138217679155408 -> 138217679155984
	138221981906704 [label="stages.3.1.transformer.2.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	138221981906704 -> 138217679155408
	138217679155408 [label=AccumulateGrad]
	138217804216016 -> 138217804216112
	138221981902480 [label="stages.3.1.transformer.2.mlp.fc2.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	138221981902480 -> 138217804216016
	138217804216016 [label=AccumulateGrad]
	138217679154784 -> 138217804216112
	138221981908336 [label="stages.3.1.transformer.2.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	138221981908336 -> 138217679154784
	138217679154784 [label=AccumulateGrad]
	138217804216256 -> 138217804216352
	138217804216256 [label=ConvolutionBackward0]
	138217804216160 -> 138217804216256
	138217804216160 [label=MulBackward0]
	138217679156512 -> 138217804216160
	138217679156512 [label=ReluBackward0]
	138217679156800 -> 138217679156512
	138217679156800 [label=SplitWithSizesBackward0]
	138217679156896 -> 138217679156800
	138217679156896 [label=ConvolutionBackward0]
	138217679157040 -> 138217679156896
	138217679157040 [label=NativeGroupNormBackward0]
	138217804216208 -> 138217679157040
	138217679158768 -> 138217679157040
	138221981903632 [label="stages.3.1.transformer.3.norm1.weight
 (96)" fillcolor=lightblue]
	138221981903632 -> 138217679158768
	138217679158768 [label=AccumulateGrad]
	138217679157424 -> 138217679157040
	138221981905552 [label="stages.3.1.transformer.3.norm1.bias
 (96)" fillcolor=lightblue]
	138221981905552 -> 138217679157424
	138217679157424 [label=AccumulateGrad]
	138217679156944 -> 138217679156896
	138221981907952 [label="stages.3.1.transformer.3.attn.qkv_proj.weight
 (193, 96, 1, 1)" fillcolor=lightblue]
	138221981907952 -> 138217679156944
	138217679156944 [label=AccumulateGrad]
	138217679156560 -> 138217679156896
	138221981907472 [label="stages.3.1.transformer.3.attn.qkv_proj.bias
 (193)" fillcolor=lightblue]
	138221981907472 -> 138217679156560
	138217679156560 [label=AccumulateGrad]
	138217679156464 -> 138217804216160
	138217679156464 [label=ExpandBackward0]
	138217679157856 -> 138217679156464
	138217679157856 [label=SumBackward1]
	138217679158864 -> 138217679157856
	138217679158864 [label=MulBackward0]
	138217679156800 -> 138217679158864
	138217679144992 -> 138217679158864
	138217679144992 [label=SoftmaxBackward0]
	138217679156800 -> 138217679144992
	138217679155600 -> 138217804216256
	138221981900944 [label="stages.3.1.transformer.3.attn.out_proj.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	138221981900944 -> 138217679155600
	138217679155600 [label=AccumulateGrad]
	138217679156032 -> 138217804216256
	138221981892880 [label="stages.3.1.transformer.3.attn.out_proj.bias
 (96)" fillcolor=lightblue]
	138221981892880 -> 138217679156032
	138217679156032 [label=AccumulateGrad]
	138217804216400 -> 138217804216736
	138217804216400 [label=ConvolutionBackward0]
	138217804216304 -> 138217804216400
	138217804216304 [label=SiluBackward0]
	138217679143456 -> 138217804216304
	138217679143456 [label=ConvolutionBackward0]
	138217679156608 -> 138217679143456
	138217679156608 [label=NativeGroupNormBackward0]
	138217804216352 -> 138217679156608
	138217679145136 -> 138217679156608
	138221981908432 [label="stages.3.1.transformer.3.norm2.weight
 (96)" fillcolor=lightblue]
	138221981908432 -> 138217679145136
	138217679145136 [label=AccumulateGrad]
	138217679143504 -> 138217679156608
	138221981908624 [label="stages.3.1.transformer.3.norm2.bias
 (96)" fillcolor=lightblue]
	138221981908624 -> 138217679143504
	138217679143504 [label=AccumulateGrad]
	138217679143408 -> 138217679143456
	138221981893264 [label="stages.3.1.transformer.3.mlp.fc1.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	138221981893264 -> 138217679143408
	138217679143408 [label=AccumulateGrad]
	138217679157472 -> 138217679143456
	138228947453136 [label="stages.3.1.transformer.3.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	138228947453136 -> 138217679157472
	138217679157472 [label=AccumulateGrad]
	138217679156320 -> 138217804216400
	138228947455728 [label="stages.3.1.transformer.3.mlp.fc2.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	138228947455728 -> 138217679156320
	138217679156320 [label=AccumulateGrad]
	138217679156272 -> 138217804216400
	138228947458320 [label="stages.3.1.transformer.3.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	138228947458320 -> 138217679156272
	138217679156272 [label=AccumulateGrad]
	138217804216832 -> 138217804216880
	138228947458128 [label="stages.3.1.norm.weight
 (96)" fillcolor=lightblue]
	138228947458128 -> 138217804216832
	138217804216832 [label=AccumulateGrad]
	138217804217312 -> 138217804216880
	138228947458992 [label="stages.3.1.norm.bias
 (96)" fillcolor=lightblue]
	138228947458992 -> 138217804217312
	138217804217312 [label=AccumulateGrad]
	138217804217408 -> 138217804217744
	138228947452752 [label="stages.3.1.conv_proj.conv.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	138228947452752 -> 138217804217408
	138217804217408 [label=AccumulateGrad]
	138217804217792 -> 138217804218080
	138228947452080 [label="stages.3.1.conv_proj.bn.weight
 (192)" fillcolor=lightblue]
	138228947452080 -> 138217804217792
	138217804217792 [label=AccumulateGrad]
	138217804217936 -> 138217804218080
	138228947453616 [label="stages.3.1.conv_proj.bn.bias
 (192)" fillcolor=lightblue]
	138228947453616 -> 138217804217936
	138217804217936 [label=AccumulateGrad]
	138217804218128 -> 138219867341776
	138228947457744 [label="stages.4.0.conv1_1x1.conv.weight
 (384, 192, 1, 1)" fillcolor=lightblue]
	138228947457744 -> 138217804218128
	138217804218128 [label=AccumulateGrad]
	138219867342784 -> 138219867341872
	138228947453040 [label="stages.4.0.conv1_1x1.bn.weight
 (384)" fillcolor=lightblue]
	138228947453040 -> 138219867342784
	138219867342784 [label=AccumulateGrad]
	138217804218320 -> 138219867341872
	138228947451984 [label="stages.4.0.conv1_1x1.bn.bias
 (384)" fillcolor=lightblue]
	138228947451984 -> 138217804218320
	138217804218320 [label=AccumulateGrad]
	138219867345904 -> 138219867346096
	138228947454384 [label="stages.4.0.conv2_kxk.conv.weight
 (384, 1, 3, 3)" fillcolor=lightblue]
	138228947454384 -> 138219867345904
	138219867345904 [label=AccumulateGrad]
	138219867346144 -> 138219867346192
	138228947455344 [label="stages.4.0.conv2_kxk.bn.weight
 (384)" fillcolor=lightblue]
	138228947455344 -> 138219867346144
	138219867346144 [label=AccumulateGrad]
	138219867346288 -> 138219867346192
	138228947457840 [label="stages.4.0.conv2_kxk.bn.bias
 (384)" fillcolor=lightblue]
	138228947457840 -> 138219867346288
	138219867346288 [label=AccumulateGrad]
	138219867347056 -> 138219867347296
	138228947453904 [label="stages.4.0.conv3_1x1.conv.weight
 (256, 384, 1, 1)" fillcolor=lightblue]
	138228947453904 -> 138219867347056
	138219867347056 [label=AccumulateGrad]
	138219867347344 -> 138219867347440
	138228947456400 [label="stages.4.0.conv3_1x1.bn.weight
 (256)" fillcolor=lightblue]
	138228947456400 -> 138219867347344
	138219867347344 [label=AccumulateGrad]
	138219867347392 -> 138219867347440
	138228947457168 [label="stages.4.0.conv3_1x1.bn.bias
 (256)" fillcolor=lightblue]
	138228947457168 -> 138219867347392
	138219867347392 [label=AccumulateGrad]
	138219867347536 -> 138219867348064
	138228947457552 [label="stages.4.1.conv_kxk.conv.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	138228947457552 -> 138219867347536
	138219867347536 [label=AccumulateGrad]
	138219867348976 -> 138219867349072
	138228947454960 [label="stages.4.1.conv_kxk.bn.weight
 (256)" fillcolor=lightblue]
	138228947454960 -> 138219867348976
	138219867348976 [label=AccumulateGrad]
	138219867350176 -> 138219867349072
	138228947457072 [label="stages.4.1.conv_kxk.bn.bias
 (256)" fillcolor=lightblue]
	138228947457072 -> 138219867350176
	138219867350176 [label=AccumulateGrad]
	138219867350416 -> 138219867350464
	138228947455056 [label="stages.4.1.conv_1x1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	138228947455056 -> 138219867350416
	138219867350416 [label=AccumulateGrad]
	138219867355600 -> 138219867353584
	138219867355600 [label=ConvolutionBackward0]
	138219867350896 -> 138219867355600
	138219867350896 [label=MulBackward0]
	138219867347920 -> 138219867350896
	138219867347920 [label=ReluBackward0]
	138219867347152 -> 138219867347920
	138219867347152 [label=SplitWithSizesBackward0]
	138219867346000 -> 138219867347152
	138219867346000 [label=ConvolutionBackward0]
	138219867342256 -> 138219867346000
	138219867342256 [label=NativeGroupNormBackward0]
	138219867355552 -> 138219867342256
	138217804217600 -> 138219867342256
	138228947452464 [label="stages.4.1.transformer.0.norm1.weight
 (128)" fillcolor=lightblue]
	138228947452464 -> 138217804217600
	138217804217600 [label=AccumulateGrad]
	138217804217072 -> 138219867342256
	138228947457264 [label="stages.4.1.transformer.0.norm1.bias
 (128)" fillcolor=lightblue]
	138228947457264 -> 138217804217072
	138217804217072 [label=AccumulateGrad]
	138219867346240 -> 138219867346000
	138228947456496 [label="stages.4.1.transformer.0.attn.qkv_proj.weight
 (257, 128, 1, 1)" fillcolor=lightblue]
	138228947456496 -> 138219867346240
	138219867346240 [label=AccumulateGrad]
	138219867349408 -> 138219867346000
	138228947454864 [label="stages.4.1.transformer.0.attn.qkv_proj.bias
 (257)" fillcolor=lightblue]
	138228947454864 -> 138219867349408
	138219867349408 [label=AccumulateGrad]
	138219867347872 -> 138219867350896
	138219867347872 [label=ExpandBackward0]
	138219867347248 -> 138219867347872
	138219867347248 [label=SumBackward1]
	138217804217648 -> 138219867347248
	138217804217648 [label=MulBackward0]
	138219867347152 -> 138217804217648
	138217804217264 -> 138217804217648
	138217804217264 [label=SoftmaxBackward0]
	138219867347152 -> 138217804217264
	138219867350992 -> 138219867355600
	138228947455440 [label="stages.4.1.transformer.0.attn.out_proj.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	138228947455440 -> 138219867350992
	138219867350992 [label=AccumulateGrad]
	138219867351952 -> 138219867355600
	138228947455824 [label="stages.4.1.transformer.0.attn.out_proj.bias
 (128)" fillcolor=lightblue]
	138228947455824 -> 138219867351952
	138219867351952 [label=AccumulateGrad]
	138219867352144 -> 138219867352000
	138219867352144 [label=ConvolutionBackward0]
	138219867355456 -> 138219867352144
	138219867355456 [label=SiluBackward0]
	138217804216640 -> 138219867355456
	138217804216640 [label=ConvolutionBackward0]
	138217804218176 -> 138217804216640
	138217804218176 [label=NativeGroupNormBackward0]
	138219867353584 -> 138217804218176
	138217679143552 -> 138217804218176
	138228947453520 [label="stages.4.1.transformer.0.norm2.weight
 (128)" fillcolor=lightblue]
	138228947453520 -> 138217679143552
	138217679143552 [label=AccumulateGrad]
	138217679145040 -> 138217804218176
	138228947454000 [label="stages.4.1.transformer.0.norm2.bias
 (128)" fillcolor=lightblue]
	138228947454000 -> 138217679145040
	138217679145040 [label=AccumulateGrad]
	138217804216976 -> 138217804216640
	138228947452848 [label="stages.4.1.transformer.0.mlp.fc1.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	138228947452848 -> 138217804216976
	138217804216976 [label=AccumulateGrad]
	138217804218272 -> 138217804216640
	138228947454576 [label="stages.4.1.transformer.0.mlp.fc1.bias
 (256)" fillcolor=lightblue]
	138228947454576 -> 138217804218272
	138217804218272 [label=AccumulateGrad]
	138219867350848 -> 138219867352144
	138228947453424 [label="stages.4.1.transformer.0.mlp.fc2.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	138228947453424 -> 138219867350848
	138219867350848 [label=AccumulateGrad]
	138219867355648 -> 138219867352144
	138228947454288 [label="stages.4.1.transformer.0.mlp.fc2.bias
 (128)" fillcolor=lightblue]
	138228947454288 -> 138219867355648
	138219867355648 [label=AccumulateGrad]
	138219867354736 -> 138219867354832
	138219867354736 [label=ConvolutionBackward0]
	138219867345952 -> 138219867354736
	138219867345952 [label=MulBackward0]
	138217679145184 -> 138219867345952
	138217679145184 [label=ReluBackward0]
	138217679145328 -> 138217679145184
	138217679145328 [label=SplitWithSizesBackward0]
	138217679145424 -> 138217679145328
	138217679145424 [label=ConvolutionBackward0]
	138217679145520 -> 138217679145424
	138217679145520 [label=NativeGroupNormBackward0]
	138219867352000 -> 138217679145520
	138217679155168 -> 138217679145520
	138228947456304 [label="stages.4.1.transformer.1.norm1.weight
 (128)" fillcolor=lightblue]
	138228947456304 -> 138217679155168
	138217679155168 [label=AccumulateGrad]
	138217679155216 -> 138217679145520
	138228947458032 [label="stages.4.1.transformer.1.norm1.bias
 (128)" fillcolor=lightblue]
	138228947458032 -> 138217679155216
	138217679155216 [label=AccumulateGrad]
	138217679145472 -> 138217679145424
	138228947455920 [label="stages.4.1.transformer.1.attn.qkv_proj.weight
 (257, 128, 1, 1)" fillcolor=lightblue]
	138228947455920 -> 138217679145472
	138217679145472 [label=AccumulateGrad]
	138217679145232 -> 138217679145424
	138228947455248 [label="stages.4.1.transformer.1.attn.qkv_proj.bias
 (257)" fillcolor=lightblue]
	138228947455248 -> 138217679145232
	138217679145232 [label=AccumulateGrad]
	138217679143600 -> 138219867345952
	138217679143600 [label=ExpandBackward0]
	138217679143360 -> 138217679143600
	138217679143360 [label=SumBackward1]
	138217679155072 -> 138217679143360
	138217679155072 [label=MulBackward0]
	138217679145328 -> 138217679155072
	138217679154928 -> 138217679155072
	138217679154928 [label=SoftmaxBackward0]
	138217679145328 -> 138217679154928
	138219867352048 -> 138219867354736
	138228947453328 [label="stages.4.1.transformer.1.attn.out_proj.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	138228947453328 -> 138219867352048
	138219867352048 [label=AccumulateGrad]
	138217804217168 -> 138219867354736
	138228947456976 [label="stages.4.1.transformer.1.attn.out_proj.bias
 (128)" fillcolor=lightblue]
	138228947456976 -> 138217804217168
	138217804217168 [label=AccumulateGrad]
	138219867355312 -> 138219867355408
	138219867355312 [label=ConvolutionBackward0]
	138217804216448 -> 138219867355312
	138217804216448 [label=SiluBackward0]
	138217679153200 -> 138217804216448
	138217679153200 [label=ConvolutionBackward0]
	138217679145280 -> 138217679153200
	138217679145280 [label=NativeGroupNormBackward0]
	138219867354832 -> 138217679145280
	138217679154832 -> 138217679145280
	138228947454192 [label="stages.4.1.transformer.1.norm2.weight
 (128)" fillcolor=lightblue]
	138228947454192 -> 138217679154832
	138217679154832 [label=AccumulateGrad]
	138217679153152 -> 138217679145280
	138228947456880 [label="stages.4.1.transformer.1.norm2.bias
 (128)" fillcolor=lightblue]
	138228947456880 -> 138217679153152
	138217679153152 [label=AccumulateGrad]
	138217679154880 -> 138217679153200
	138228947456016 [label="stages.4.1.transformer.1.mlp.fc1.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	138228947456016 -> 138217679154880
	138217679154880 [label=AccumulateGrad]
	138217679156080 -> 138217679153200
	138228947454480 [label="stages.4.1.transformer.1.mlp.fc1.bias
 (256)" fillcolor=lightblue]
	138228947454480 -> 138217679156080
	138217679156080 [label=AccumulateGrad]
	138219867355072 -> 138219867355312
	138228947454096 [label="stages.4.1.transformer.1.mlp.fc2.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	138228947454096 -> 138219867355072
	138219867355072 [label=AccumulateGrad]
	138217679156848 -> 138219867355312
	138228947452656 [label="stages.4.1.transformer.1.mlp.fc2.bias
 (128)" fillcolor=lightblue]
	138228947452656 -> 138217679156848
	138217679156848 [label=AccumulateGrad]
	138219867353680 -> 138219867354496
	138219867353680 [label=ConvolutionBackward0]
	138219867355360 -> 138219867353680
	138219867355360 [label=MulBackward0]
	138217679152672 -> 138219867355360
	138217679152672 [label=ReluBackward0]
	138217679152720 -> 138217679152672
	138217679152720 [label=SplitWithSizesBackward0]
	138217679153008 -> 138217679152720
	138217679153008 [label=ConvolutionBackward0]
	138217679152960 -> 138217679153008
	138217679152960 [label=NativeGroupNormBackward0]
	138219867355408 -> 138217679152960
	138217679143888 -> 138217679152960
	138228947458512 [label="stages.4.1.transformer.2.norm1.weight
 (128)" fillcolor=lightblue]
	138228947458512 -> 138217679143888
	138217679143888 [label=AccumulateGrad]
	138217679144032 -> 138217679152960
	138228947458416 [label="stages.4.1.transformer.2.norm1.bias
 (128)" fillcolor=lightblue]
	138228947458416 -> 138217679144032
	138217679144032 [label=AccumulateGrad]
	138217679153056 -> 138217679153008
	138228947456208 [label="stages.4.1.transformer.2.attn.qkv_proj.weight
 (257, 128, 1, 1)" fillcolor=lightblue]
	138228947456208 -> 138217679153056
	138217679153056 [label=AccumulateGrad]
	138217679155120 -> 138217679153008
	138228947455152 [label="stages.4.1.transformer.2.attn.qkv_proj.bias
 (257)" fillcolor=lightblue]
	138228947455152 -> 138217679155120
	138217679155120 [label=AccumulateGrad]
	138217679154976 -> 138219867355360
	138217679154976 [label=ExpandBackward0]
	138217679152864 -> 138217679154976
	138217679152864 [label=SumBackward1]
	138217679143936 -> 138217679152864
	138217679143936 [label=MulBackward0]
	138217679152720 -> 138217679143936
	138217679143984 -> 138217679143936
	138217679143984 [label=SoftmaxBackward0]
	138217679152720 -> 138217679143984
	138217679155024 -> 138219867353680
	138228947452944 [label="stages.4.1.transformer.2.attn.out_proj.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	138228947452944 -> 138217679155024
	138217679155024 [label=AccumulateGrad]
	138217679145376 -> 138219867353680
	138228947458800 [label="stages.4.1.transformer.2.attn.out_proj.bias
 (128)" fillcolor=lightblue]
	138228947458800 -> 138217679145376
	138217679145376 [label=AccumulateGrad]
	138219867354208 -> 138217804090768
	138219867354208 [label=ConvolutionBackward0]
	138219867354544 -> 138219867354208
	138219867354544 [label=SiluBackward0]
	138217679144128 -> 138219867354544
	138217679144128 [label=ConvolutionBackward0]
	138217679152816 -> 138217679144128
	138217679152816 [label=NativeGroupNormBackward0]
	138219867354496 -> 138217679152816
	138217679152336 -> 138217679152816
	138228947459184 [label="stages.4.1.transformer.2.norm2.weight
 (128)" fillcolor=lightblue]
	138228947459184 -> 138217679152336
	138217679152336 [label=AccumulateGrad]
	138217679152528 -> 138217679152816
	138228947452272 [label="stages.4.1.transformer.2.norm2.bias
 (128)" fillcolor=lightblue]
	138228947452272 -> 138217679152528
	138217679152528 [label=AccumulateGrad]
	138217679144080 -> 138217679144128
	138228947458896 [label="stages.4.1.transformer.2.mlp.fc1.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	138228947458896 -> 138217679144080
	138217679144080 [label=AccumulateGrad]
	138217679144176 -> 138217679144128
	138228947457936 [label="stages.4.1.transformer.2.mlp.fc1.bias
 (256)" fillcolor=lightblue]
	138228947457936 -> 138217679144176
	138217679144176 [label=AccumulateGrad]
	138217679152624 -> 138219867354208
	138228947458704 [label="stages.4.1.transformer.2.mlp.fc2.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	138228947458704 -> 138217679152624
	138217679152624 [label=AccumulateGrad]
	138217679152768 -> 138219867354208
	138228947452560 [label="stages.4.1.transformer.2.mlp.fc2.bias
 (128)" fillcolor=lightblue]
	138228947452560 -> 138217679152768
	138217679152768 [label=AccumulateGrad]
	138217804089088 -> 138217804089136
	138228947462928 [label="stages.4.1.norm.weight
 (128)" fillcolor=lightblue]
	138228947462928 -> 138217804089088
	138217804089088 [label=AccumulateGrad]
	138217804093360 -> 138217804089136
	138228947462832 [label="stages.4.1.norm.bias
 (128)" fillcolor=lightblue]
	138228947462832 -> 138217804093360
	138217804093360 [label=AccumulateGrad]
	138217804090528 -> 138217804088320
	138228947464464 [label="stages.4.1.conv_proj.conv.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	138228947464464 -> 138217804090528
	138217804090528 [label=AccumulateGrad]
	138217804088224 -> 138217804090288
	138228947463984 [label="stages.4.1.conv_proj.bn.weight
 (256)" fillcolor=lightblue]
	138228947463984 -> 138217804088224
	138217804088224 [label=AccumulateGrad]
	138217804089328 -> 138217804090288
	138228947465040 [label="stages.4.1.conv_proj.bn.bias
 (256)" fillcolor=lightblue]
	138228947465040 -> 138217804089328
	138217804089328 [label=AccumulateGrad]
	138217804087936 -> 138217804090864
	138217804087936 [label=TBackward0]
	138217804088272 -> 138217804087936
	138228947466768 [label="head.fc.weight
 (7, 256)" fillcolor=lightblue]
	138228947466768 -> 138217804088272
	138217804088272 [label=AccumulateGrad]
	138217804090864 -> 138217673228944
}
